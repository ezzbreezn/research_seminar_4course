{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXseAgMvU7bE"
      },
      "outputs": [],
      "source": [
        "!pip install optuna rtdl category_encoders ruamel.yaml einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import fmin\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pathlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from tqdm import tqdm, trange\n",
        "from typing import Optional, Sequence, Tuple, Union, Any, Dict, List\n",
        "from copy import deepcopy\n",
        "import enum\n",
        "import optuna\n",
        "import rtdl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import category_encoders as ce\n",
        "import ruamel.yaml\n",
        "import math\n",
        "from collections import OrderedDict, defaultdict\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from scipy.spatial import distance_matrix\n",
        "from scipy.linalg import qr\n",
        "from torch.autograd import Function\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "yZ_7TzkyVCNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_global_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ],
      "metadata": {
        "id": "PjyPp1z8VFSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "3V6_VweFVGDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessings"
      ],
      "metadata": {
        "id": "SXNA6iSyVIe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sin(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.cat([torch.cos(x), torch.sin(x)], -1)\n",
        "\n",
        "def positional(d, pos):\n",
        "    return torch.Tensor([np.sin(1/10000 ** (2 * int(i / 2) / d) * pos) if i % 2 == 0 else \n",
        "                       np.cos(1/10000 ** (2 * int(i / 2) / d) * pos) for i in range(d)])\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, n_features: int = 5000, dropout: float = 0.1, tf: bool=False):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.tf = tf\n",
        "        self.d_model = d_model\n",
        "        position = torch.arange(n_features).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(n_features, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.pe = pe\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.tf:\n",
        "            x = x + self.pe.unsqueeze(0)\n",
        "        else:\n",
        "            x = x + self.pe.view(1, -1)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Periodic(nn.Module):\n",
        "    def __init__(self, n_features: int, n: int, sigma: float, trainable: bool, initialization: str, tf: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.tf = tf\n",
        "        if initialization == 'log-linear':\n",
        "            coefficients = sigma ** (torch.arange(n) / n)\n",
        "            coefficients = coefficients[None].repeat(n_features, 1)\n",
        "        else:\n",
        "            assert initialization == 'normal'\n",
        "            coefficients = torch.normal(0.0, sigma, (n_features, n))\n",
        "        if trainable:\n",
        "            self.coefficients = nn.Parameter(coefficients) \n",
        "        else:\n",
        "            self.register_buffer('coefficients', coefficients)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.tf:\n",
        "            return cos_sin(2 * np.pi * self.coefficients[None] * x[..., None]).view(-1, 2 * x.shape[1] * self.coefficients.shape[1])\n",
        "        else:\n",
        "            return cos_sin(2 * np.pi * self.coefficients[None] * x[..., None]).view(x.shape[0], x.shape[1], 2 * self.coefficients.shape[1])\n",
        "    \n",
        "    \n",
        "class NLinear(nn.Module):\n",
        "    def __init__(self, n: int, d_in: int, d_out: int, bias: bool = True) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(n, d_in, d_out))\n",
        "        self.bias = nn.Parameter(torch.Tensor(n, d_out)) if bias else None\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                layer = nn.Linear(d_in, d_out)\n",
        "                self.weight[i] = layer.weight.T\n",
        "                if self.bias is not None:\n",
        "                    self.bias[i] = layer.bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3:\n",
        "            x = x[..., None] * self.weight[None]\n",
        "            x = x.sum(-2)\n",
        "            if self.bias is not None:\n",
        "                x = x + self.bias[None]\n",
        "            return x\n",
        "    \n",
        "\n",
        "class FeaturesTokenizer(nn.Module):\n",
        "    def __init__(self, n_features: int, d_embedding: int, tf: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.tf = tf\n",
        "        self.first_layer = rtdl.NumericalFeatureTokenizer(n_features, d_embedding, True, 'uniform')\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.first_layer(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "class LinearEmbeddings(nn.Module):\n",
        "    def __init__(self, n_layers, n_features, d_embeddings, tf):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.tf = tf\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layers):\n",
        "            if i == 0:\n",
        "                self.layers.append(rtdl.NumericalFeatureTokenizer(n_features, d_embeddings[i], False, 'uniform'))\n",
        "            else:\n",
        "                self.layers.append(NLinear(n_features, d_embeddings[i-1], d_embeddings[i], False))\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "            x = self.leaky_relu(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x    \n",
        "    \n",
        "    \n",
        "class AutoDis(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_features: int, d_embedding: int, n_meta_embeddings: int, temperature: float, tf: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.first_layer = rtdl.NumericalFeatureTokenizer(\n",
        "            n_features,\n",
        "            n_meta_embeddings,\n",
        "            False,\n",
        "            'uniform',\n",
        "        )\n",
        "        self.tf = tf\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.second_layer = NLinear(\n",
        "            n_features, n_meta_embeddings, n_meta_embeddings, False\n",
        "        )\n",
        "        self.softmax = nn.Softmax(-1)\n",
        "        self.temperature = temperature\n",
        "        self.third_layer = NLinear(\n",
        "            n_features, n_meta_embeddings, d_embedding, False\n",
        "        )\n",
        "        nn.init.uniform_(self.third_layer.weight, 0.01)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.first_layer(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.second_layer(x)\n",
        "        x = self.softmax(x / self.temperature)\n",
        "        x = self.third_layer(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "    \n",
        "class SoftEmbedding(torch.nn.Module):\n",
        "    def __init__(self, num_embeddings, embeddings_dim, emb_initializer=None, tf=True):\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.embedding_table = torch.nn.Embedding(num_embeddings, embeddings_dim)\n",
        "        if emb_initializer:\n",
        "            emb_initializer(self.embedding_table.weight)\n",
        "        self.projection_layer = torch.nn.Linear(1, num_embeddings, bias=True)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.tf = tf\n",
        "\n",
        "    def forward(self, input_numeric):\n",
        "        input_numeric = input_numeric.unsqueeze(-1)\n",
        "        weights = self.softmax(self.projection_layer(input_numeric))\n",
        "        soft_one_hot_embeddings = (weights.unsqueeze(-1) * self.embedding_table.weight).sum(-2)\n",
        "        if not self.tf:\n",
        "            return soft_one_hot_embeddings.view(soft_one_hot_embeddings.shape[0], -1)\n",
        "        else:\n",
        "            return soft_one_hot_embeddings\n",
        "\n",
        "\n",
        "class EntmaxBisectFunction(Function):\n",
        "    @classmethod\n",
        "    def _gp(cls, x, alpha):\n",
        "        return x ** (alpha - 1)\n",
        "\n",
        "    @classmethod\n",
        "    def _gp_inv(cls, y, alpha):\n",
        "        return y ** (1 / (alpha - 1))\n",
        "\n",
        "    @classmethod\n",
        "    def _p(cls, X, alpha):\n",
        "        return cls._gp_inv(torch.clamp(X, min=0), alpha)\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\n",
        "\n",
        "        if not isinstance(alpha, torch.Tensor):\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=X.device)\n",
        "\n",
        "        alpha_shape = list(X.shape)\n",
        "        alpha_shape[dim] = 1\n",
        "        alpha = alpha.expand(*alpha_shape)\n",
        "\n",
        "        ctx.alpha = alpha\n",
        "        ctx.dim = dim\n",
        "        d = X.shape[dim]\n",
        "\n",
        "        X = X * (alpha - 1)\n",
        "\n",
        "        max_val, _ = X.max(dim=dim, keepdim=True)\n",
        "\n",
        "        tau_lo = max_val - cls._gp(1, alpha)\n",
        "        tau_hi = max_val - cls._gp(1 / d, alpha)\n",
        "\n",
        "        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\n",
        "\n",
        "        dm = tau_hi - tau_lo\n",
        "\n",
        "        for it in range(n_iter):\n",
        "\n",
        "            dm /= 2\n",
        "            tau_m = tau_lo + dm\n",
        "            p_m = cls._p(X - tau_m, alpha)\n",
        "            f_m = p_m.sum(dim) - 1\n",
        "\n",
        "            mask = (f_m * f_lo >= 0).unsqueeze(dim)\n",
        "            tau_lo = torch.where(mask, tau_m, tau_lo)\n",
        "\n",
        "        if ensure_sum_one:\n",
        "            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\n",
        "\n",
        "        ctx.save_for_backward(p_m)\n",
        "\n",
        "        return p_m\n",
        "\n",
        "    @classmethod\n",
        "    def backward(cls, ctx, dY):\n",
        "        Y, = ctx.saved_tensors\n",
        "\n",
        "        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\n",
        "\n",
        "        dX = dY * gppr\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
        "        q = q.unsqueeze(ctx.dim)\n",
        "        dX -= q * gppr\n",
        "\n",
        "        d_alpha = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "\n",
        "            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))\n",
        "            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "\n",
        "            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\n",
        "            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\n",
        "            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "\n",
        "        return dX, d_alpha, None, None, None\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\n",
        "    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\n",
        "\n",
        "        \n",
        "        \n",
        "class EntmaxBisect(nn.Module):\n",
        "    def __init__(self, alpha=1.5, dim=-1, n_iter=50):\n",
        "        self.dim = dim\n",
        "        self.n_iter = n_iter\n",
        "        self.alpha = alpha\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return entmax_bisect(\n",
        "            X, alpha=self.alpha, dim=self.dim, n_iter=self.n_iter\n",
        "        )\n",
        "  \n",
        "        \n",
        "class SparseAttLayer(nn.Module):\n",
        "    def __init__(self, nhead: int, nfield: int, nemb: int, d_k: int, nhid: int, alpha: float = 1.5):\n",
        "        \"\"\" Multi-Head Sparse Attention Layer \"\"\"\n",
        "        super(SparseAttLayer, self).__init__()\n",
        "        self.sparsemax = nn.Softmax(dim=-1) if alpha == 1. \\\n",
        "            else EntmaxBisect(alpha, dim=-1)\n",
        "\n",
        "        self.scale = d_k ** -0.5\n",
        "        self.bilinear_w = nn.Parameter(torch.zeros(nhead, nemb, d_k))                   \n",
        "        self.query = nn.Parameter(torch.zeros(nhead, nhid, d_k))                        \n",
        "        self.values = nn.Parameter(torch.zeros(nhead, nhid, nfield))                    \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.xavier_uniform_(self.bilinear_w, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.query, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.values, gain=1.414)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x                                                                        \n",
        "        att_gates = torch.einsum('bfx,kxy,koy->bkof',\n",
        "                                 keys, self.bilinear_w, self.query) * self.scale        \n",
        "        sparse_gates = self.sparsemax(att_gates)                                        \n",
        "        return torch.einsum('bkof,kof->bkof', sparse_gates, self.values)\n",
        "\n",
        "    \n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat, nemb):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(nfeat, nemb)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x['id'])                           \n",
        "        return emb * x['value'].unsqueeze(2)                    \n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dim_head = 16,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        dropped_attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', dropped_attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out), attn\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.Embedding(num_tokens, dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, return_attn = False):\n",
        "        x = self.embeds(x)\n",
        "\n",
        "        post_softmax_attns = []\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            attn_out, post_softmax_attn = attn(x)\n",
        "            post_softmax_attns.append(post_softmax_attn)\n",
        "\n",
        "            x = x + attn_out\n",
        "            x = ff(x) + x\n",
        "\n",
        "        if not return_attn:\n",
        "            return x\n",
        "\n",
        "        return x, torch.stack(post_softmax_attns)\n",
        "\n",
        "class MLP_TT(nn.Module):\n",
        "    def __init__(self, dims, act = None):\n",
        "        super().__init__()\n",
        "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
        "        layers = []\n",
        "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
        "            is_last = ind >= (len(dims_pairs) - 1)\n",
        "            linear = nn.Linear(dim_in, dim_out)\n",
        "            layers.append(linear)\n",
        "\n",
        "            if is_last:\n",
        "                continue\n",
        "\n",
        "            act = default(act, nn.ReLU())\n",
        "            layers.append(act)\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        categories,\n",
        "        num_continuous,\n",
        "        dim, \n",
        "        depth, \n",
        "        heads, \n",
        "        dim_head = 16,\n",
        "        dim_out = 1,\n",
        "        mlp_hidden_mults = (4, 2),\n",
        "        mlp_act = None,\n",
        "        num_special_tokens = 2,\n",
        "        continuous_mean_std = None,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_categories = len(categories)\n",
        "        self.num_unique_categories = sum(categories)\n",
        "\n",
        "        self.num_special_tokens = num_special_tokens\n",
        "        total_tokens = self.num_unique_categories + num_special_tokens\n",
        "\n",
        "        if self.num_unique_categories > 0:\n",
        "            categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
        "            categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
        "            self.register_buffer('categories_offset', categories_offset)\n",
        "\n",
        "        self.num_continuous = num_continuous\n",
        "\n",
        "        if self.num_continuous > 0:\n",
        "            if exists(continuous_mean_std):\n",
        "                assert continuous_mean_std.shape == (num_continuous, 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
        "            self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
        "\n",
        "            self.norm = nn.LayerNorm(num_continuous)\n",
        "\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            num_tokens = total_tokens,\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            heads = heads,\n",
        "            dim_head = dim_head,\n",
        "            attn_dropout = attn_dropout,\n",
        "            ff_dropout = ff_dropout\n",
        "        )\n",
        "\n",
        "        input_size = (dim * self.num_categories) + num_continuous\n",
        "        l = input_size // 8\n",
        "\n",
        "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
        "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
        "        \n",
        "        \n",
        "        self.mlp = MLP_TT(all_dimensions, act = mlp_act)\n",
        "\n",
        "    def forward(self, x_categ, x_cont, return_attn = False):\n",
        "        xs = []\n",
        "\n",
        "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
        "\n",
        "        if self.num_unique_categories > 0:\n",
        "            x_categ += self.categories_offset\n",
        "\n",
        "            x, attns = self.transformer(x_categ, return_attn = True)\n",
        "            xs.append(x)\n",
        "        if self.num_continuous > 0:\n",
        "            if exists(self.continuous_mean_std):\n",
        "                mean, std = self.continuous_mean_std.unbind(dim = -1)\n",
        "                x_cont = (x_cont - mean) / std\n",
        "\n",
        "            normed_cont = self.norm(x_cont)\n",
        "            xs.append(normed_cont)\n",
        "\n",
        "        x = torch.cat(xs, dim = -1)\n",
        "        return x\n",
        "        \n"
      ],
      "metadata": {
        "id": "z5vGnitNVKlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "JwI_aFgNVOpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _TokenInitialization(enum.Enum):\n",
        "    UNIFORM = 'uniform'\n",
        "    NORMAL = 'normal'\n",
        "\n",
        "    @classmethod\n",
        "    def from_str(cls, initialization: str) -> '_TokenInitialization':\n",
        "        try:\n",
        "            return cls(initialization)\n",
        "        except ValueError:\n",
        "            valid_values = [x.value for x in _TokenInitialization]\n",
        "            raise ValueError(f'initialization must be one of {valid_values}')\n",
        "\n",
        "    def apply(self, x: torch.Tensor, d: int) -> None:\n",
        "        d_sqrt_inv = 1 / math.sqrt(d)\n",
        "        if self == _TokenInitialization.UNIFORM:\n",
        "            nn.init.uniform_(x, a=-d_sqrt_inv, b=d_sqrt_inv)\n",
        "        elif self == _TokenInitialization.NORMAL:\n",
        "            nn.init.normal_(x, std=d_sqrt_inv)\n",
        "\n",
        "class CategoricalFeatureTokenizer(nn.Module):\n",
        "    category_offsets: torch.Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cardinalities: List[int],\n",
        "        d_token: int,\n",
        "        bias: bool,\n",
        "        initialization: str,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert cardinalities, 'cardinalities must be non-empty'\n",
        "        assert d_token > 0, 'd_token must be positive'\n",
        "        initialization_ = _TokenInitialization.from_str(initialization)\n",
        "\n",
        "        category_offsets = torch.tensor([0] + cardinalities[:-1]).cumsum(0)\n",
        "        self.register_buffer('category_offsets', category_offsets, persistent=False)\n",
        "        self.embeddings = nn.Embedding(sum(cardinalities), d_token)\n",
        "        self.bias = nn.Parameter(torch.Tensor(len(cardinalities), d_token)) if bias else None\n",
        "\n",
        "        for parameter in [self.embeddings.weight, self.bias]:\n",
        "            if parameter is not None:\n",
        "                initialization_.apply(parameter, d_token)\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        return len(self.category_offsets)\n",
        "\n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        return self.embeddings.embedding_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.embeddings(x + self.category_offsets[None])\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias[None]\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_num_features: int,\n",
        "        cat_cardinalities: List[int],\n",
        "        d_token: int,\n",
        "        preproc_type: str,\n",
        "        cat_preproc_type: str,\n",
        "        preproc_args: dict,\n",
        "        positional: bool,\n",
        "        tf: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert n_num_features >= 0, 'n_num_features must be non-negative'\n",
        "        assert (\n",
        "            n_num_features or cat_cardinalities\n",
        "        ), 'at least one of n_num_features or cat_cardinalities must be positive/non-empty'\n",
        "        self.initialization = 'uniform'\n",
        "        self.n_num_features = n_num_features\n",
        "        self.d = d_token\n",
        "        self.tf = tf\n",
        "        self.args = preproc_args\n",
        "        self.preproc_type = preproc_type\n",
        "        self.cat_preproc_type = cat_preproc_type\n",
        "        self.arm_num = None\n",
        "        self.arm_num_cat = None\n",
        "        if preproc_type == 'ARM' or preproc_type == 'ARM_Bin':\n",
        "            self.attn_layer = SparseAttLayer(8, n_num_features, d_token, d_token, 16, 1.7)\n",
        "            self.arm_num = 8 * self.d * 16 \n",
        "        if n_num_features:\n",
        "            if preproc_type == 'Periodic':\n",
        "                self.num_tokenizer = Periodic(**preproc_args)\n",
        "                self.d = self.d * 2\n",
        "            elif preproc_type == 'Linear':\n",
        "                self.num_tokenizer = LinearEmbeddings(**preproc_args)\n",
        "            elif preproc_type == 'AutoDis':\n",
        "                self.num_tokenizer = AutoDis(**preproc_args)\n",
        "            elif preproc_type == 'Tokens' or preproc_type == 'ARM':\n",
        "                self.num_tokenizer = FeaturesTokenizer(**preproc_args)\n",
        "            elif preproc_type == 'SoftEmbedding':\n",
        "                self.num_tokenizer = SoftEmbedding(**preproc_args)\n",
        "            elif preproc_type == 'BinEncoding' or preproc_type == 'ARM_Bin':\n",
        "                self.num_tokenizer = BinEncoding(**preproc_args)\n",
        "            elif preproc_type == 'None':\n",
        "                self.num_tokenizer = None\n",
        "        else:\n",
        "            self.num_tokenizer = None\n",
        "        if cat_cardinalities is None:\n",
        "            self.cat_tokenizer = None\n",
        "        elif cat_preproc_type == 'Lookup' or cat_preproc_type == 'ARM': \n",
        "            self.cat_tokenizer = (\n",
        "                CategoricalFeatureTokenizer(\n",
        "                    cat_cardinalities, self.d, True, self.initialization\n",
        "                )\n",
        "                if cat_cardinalities\n",
        "                else None\n",
        "            )\n",
        "        elif cat_preproc_type == 'TT':\n",
        "            self.cat_tokenizer = (\n",
        "                TabTransformer(\n",
        "                    num_continuous=0,\n",
        "                    categories=cat_cardinalities, \n",
        "                    dim=self.d, \n",
        "                    depth=3,\n",
        "                    heads=8,\n",
        "                )\n",
        "                if cat_cardinalities\n",
        "                else None\n",
        "            )\n",
        "        if cat_preproc_type == 'ARM':\n",
        "            self.arm_num_cat = (self.n_num_features + 8 * 16) * self.d\n",
        "            self.cat_attn_layer = SparseAttLayer(8, len(cat_cardinalities), self.d, self.d, 16, 1.7)\n",
        "        if positional:\n",
        "            self.positional = PositionalEncoding(self.d, self.n_num_features, 0.1, True)\n",
        "        else:\n",
        "            self.positional = None\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        return self.n_num_features + self.cat_tokenizer.n_tokens\n",
        "        \n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        return self.d\n",
        "\n",
        "    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        if self.num_tokenizer is None:\n",
        "            return x_num[..., None]\n",
        "        assert (\n",
        "            x_num is not None or x_cat is not None\n",
        "        ), 'At least one of x_num and x_cat must be presented'\n",
        "        assert _all_or_none(\n",
        "            [self.num_tokenizer, x_num]\n",
        "        ), 'If self.num_tokenizer is (not) None, then x_num must (not) be None'\n",
        "        assert _all_or_none(\n",
        "            [self.cat_tokenizer, x_cat]\n",
        "        ), 'If self.cat_tokenizer is (not) None, then x_cat must (not) be None'\n",
        "        x = []\n",
        "        if self.num_tokenizer is not None:\n",
        "            if self.positional is not None:\n",
        "                x.append(self.positional(self.num_tokenizer(x_num)))\n",
        "            else:\n",
        "                x.append(self.num_tokenizer(x_num))\n",
        "        if self.cat_tokenizer is not None:\n",
        "            if self.cat_preproc_type == 'TT':\n",
        "                x.append(self.cat_tokenizer(x_cat, None))\n",
        "            elif self.cat_preproc_type == 'ARM':\n",
        "                temp = self.cat_tokenizer(x_cat)\n",
        "                w = self.cat_attn_layer(temp)\n",
        "                w = torch.clamp(w, -1e5, 1e5)\n",
        "                x_cat_arm = torch.exp(torch.einsum('bfe,bkof->bkoe', temp, w))          \n",
        "                x_cat_arm = rearrange(x_cat_arm, 'b k o e -> b (k o) e')\n",
        "                x_cat_arm = torch.clamp(x_cat_arm, -1e5, 1e5)\n",
        "                x.append(x_cat_arm)\n",
        "            else:\n",
        "                x.append(self.cat_tokenizer(x_cat))\n",
        "\n",
        "        if self.preproc_type == 'ARM' or self.preproc_type == 'ARM_Bin':\n",
        "            e = x[0]\n",
        "            arm_weight = self.attn_layer(e)\n",
        "            arm_weight = torch.clamp(arm_weight, -1e5, 1e5)                             \n",
        "            x_arm = torch.exp(torch.einsum('bfe,bkof->bkoe', e, arm_weight))         \n",
        "            x_arm = rearrange(x_arm, 'b k o e -> b (k o) e')\n",
        "            x_arm = torch.clamp(x_arm, -1e5, 1e5)\n",
        "            x[0] = x_arm\n",
        "            x = torch.cat(x, dim=1)\n",
        "            if not self.tf:\n",
        "                return x.reshape(x.shape[0], -1)\n",
        "            else:\n",
        "                return x\n",
        "        if not self.tf:\n",
        "            return x[0].view(x[0].shape[0], -1) if len(x) == 1 else torch.cat(x, dim=1).view(x[0].shape[0], -1)\n",
        "        else:\n",
        "            return x[0] if len(x) == 1 else torch.cat(x, dim=1)\n"
      ],
      "metadata": {
        "id": "i9IZWxUXVMmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\n",
        "\n",
        "def reglu(x: torch.Tensor) -> torch.Tensor:\n",
        "    assert x.shape[-1] % 2 == 0\n",
        "    a, b = x.chunk(2, dim=-1)\n",
        "    return a * F.relu(b)\n",
        "\n",
        "\n",
        "def geglu(x: torch.Tensor) -> torch.Tensor:\n",
        "    assert x.shape[-1] % 2 == 0\n",
        "    a, b = x.chunk(2, dim=-1)\n",
        "    return a * F.gelu(b)\n",
        "\n",
        "\n",
        "\n",
        "ModuleType = Union[str, Callable[..., nn.Module]]\n",
        "_INTERNAL_ERROR_MESSAGE = 'Internal error. Please, open an issue.'\n",
        "\n",
        "\n",
        "def _is_glu_activation(activation: ModuleType):\n",
        "    return (\n",
        "        isinstance(activation, str)\n",
        "        and activation.endswith('GLU')\n",
        "        or activation in [ReGLU, GEGLU]\n",
        "    )\n",
        "\n",
        "\n",
        "def _all_or_none(values):\n",
        "    return all(x is None for x in values) or all(x is not None for x in values)\n",
        "\n",
        "\n",
        "class ReGLU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return reglu(x)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return geglu(x)\n"
      ],
      "metadata": {
        "id": "fYGrLvzWVThz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, stddev, device):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, din):\n",
        "        if self.training:\n",
        "            return din + torch.autograd.Variable(\n",
        "                torch.randn(din.size()).to(self.device) * self.stddev\n",
        "            )\n",
        "        return din\n",
        "\n",
        "class DenseLightBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_in,\n",
        "        n_out,\n",
        "        drop_rate=0.1,\n",
        "        noise_std=0.05,\n",
        "        act_fun=nn.ReLU,\n",
        "        use_bn=True,\n",
        "        use_noise=True,\n",
        "        use_dropout=True,\n",
        "        use_act=True,\n",
        "        device=torch.device(\"cuda:0\"),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(DenseLightBlock, self).__init__()\n",
        "        self.features = nn.Sequential(OrderedDict([]))\n",
        "\n",
        "        if use_bn:\n",
        "            self.features.add_module(\"norm\", nn.BatchNorm1d(n_in))\n",
        "        if use_dropout and drop_rate:\n",
        "            self.features.add_module(\"dropout\", nn.Dropout(p=drop_rate))\n",
        "        if use_noise:\n",
        "            self.features.add_module(\"noise\", GaussianNoise(noise_std, device))\n",
        "\n",
        "        self.features.add_module(\"dense\", nn.Linear(n_in, n_out))\n",
        "\n",
        "        if use_act:\n",
        "            self.features.add_module(\"act\", act_fun())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for name, layer in self.features.named_children():\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DenseLightModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_in,\n",
        "        n_out=1,\n",
        "        hidden_size=[\n",
        "            512,\n",
        "            512,\n",
        "            512,\n",
        "        ],\n",
        "        drop_rate=[\n",
        "            0.1,\n",
        "            0.1,\n",
        "            0.1,\n",
        "        ],\n",
        "        act_fun=nn.ReLU,\n",
        "        noise_std=0.05,\n",
        "        bias=None,\n",
        "        num_init_features=None,\n",
        "        use_bn=True,\n",
        "        use_noise=True,\n",
        "        use_dropout=True,\n",
        "        use_act=True,\n",
        "        concat_input=True,\n",
        "        device=torch.device(\"cuda:0\"),\n",
        "        preproc=False,\n",
        "        preproc_type=None,\n",
        "        preproc_args=None,\n",
        "        positional=False,\n",
        "        tokenizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(DenseLightModel, self).__init__()\n",
        "        \n",
        "        if isinstance(drop_rate, float):\n",
        "            drop_rate = [drop_rate] * len(hidden_size)\n",
        "            \n",
        "        assert len(hidden_size) == len(\n",
        "            drop_rate\n",
        "        ), \"Wrong number hidden_sizes/drop_rates. Must be equal.\"\n",
        "\n",
        "        self.concat_input = concat_input\n",
        "        num_features = n_in if num_init_features is None else num_init_features\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([]))\n",
        "        self.d_embedding = None\n",
        "        self.tokenizer = tokenizer\n",
        "        if tokenizer is not None:\n",
        "            self.features.add_module('preproc', tokenizer)\n",
        "            if tokenizer.arm_num is not None:\n",
        "                num_features = tokenizer.arm_num\n",
        "            elif tokenizer.arm_num_cat is not None:\n",
        "                num_features = tokenizer.arm_num_cat\n",
        "            else:\n",
        "                num_features = num_features * tokenizer.d\n",
        "            self.d_embedding = tokenizer.d\n",
        "        if preproc:\n",
        "            if preproc_type == 'Periodic':\n",
        "                self.features.add_module('preproc', Periodic(**preproc_args))\n",
        "                num_features = 2 * num_features * preproc_args['n']\n",
        "                self.d_embedding = preproc_args['n']\n",
        "            elif preproc_type == 'Linear':\n",
        "                self.features.add_module('preproc', LinearEmbeddings(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embeddings'][-1]\n",
        "                self.d_embedding = preproc_args['d_embeddings'][-1]\n",
        "            elif preproc_type == 'AutoDis':\n",
        "                self.features.add_module('preproc', AutoDis(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embedding']\n",
        "                self.d_embedding = preproc_args['d_embedding']\n",
        "            elif preproc_type == 'Tokens':\n",
        "                self.features.add_module('preproc', FeaturesTokenizer(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embedding']\n",
        "                self.d_embedding = preproc_args['d_embedding']\n",
        "            elif preproc_type == 'SoftEmbedding':\n",
        "                self.features.add_module('preproc', SoftEmbedding(**preproc_args))\n",
        "                num_features = num_features * preproc_args['embeddings_dim']\n",
        "                self.d_embedding = preproc_args['embeddings_dim']\n",
        "        if positional:\n",
        "            self.features.add_module('positional', PositionalEncoding(self.d_embedding, n_in, 0.1, False))\n",
        "        if num_init_features is not None:\n",
        "            self.features.add_module(\"dense0\", nn.Linear(n_in, num_features))\n",
        "\n",
        "        for i, hid_size in enumerate(hidden_size):\n",
        "            block = DenseLightBlock(\n",
        "                n_in=num_features,\n",
        "                n_out=hid_size,\n",
        "                drop_rate=drop_rate[i] if use_dropout else 0,\n",
        "                noise_std=noise_std,\n",
        "                act_fun=act_fun,\n",
        "                use_bn=use_bn,\n",
        "                use_noise=use_noise,\n",
        "                use_dropout=use_dropout,\n",
        "                use_act=use_act,\n",
        "                device=device,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "\n",
        "            if concat_input:\n",
        "                num_features = n_in + hid_size\n",
        "            else:\n",
        "                num_features = hid_size\n",
        "\n",
        "        num_features = hidden_size[-1]\n",
        "        self.fc = nn.Linear(num_features, n_out)\n",
        "\n",
        "        if bias is not None:\n",
        "            print(\"init bias!\")\n",
        "            bias = torch.Tensor(bias)\n",
        "            self.fc.bias.data = bias\n",
        "            self.fc.weight.data = torch.zeros(n_out, num_features, requires_grad=True)\n",
        "\n",
        "    def forward(self, x, x_cat=None):\n",
        "        for name, layer in self.features.named_children():\n",
        "            if name == 'preproc' and self.tokenizer is not None:\n",
        "                x = layer(x, x_cat)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(DenseLightModel):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MLP, self).__init__(*args, **{**kwargs, **{\"concat_input\": False}})\n",
        "\n",
        "    def forward(self, x, x_cat=None):\n",
        "        return super(MLP, self).forward(x, x_cat)\n"
      ],
      "metadata": {
        "id": "MJu7YsvdVUd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "DGkw2QrLVRhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric():\n",
        "    def __init__(self, metric, higher_is_better=True, name='name', optimize=False, discrete=False, **kwargs):\n",
        "        self.name = name\n",
        "        self.higher_is_better = higher_is_better\n",
        "        self.optimize = optimize\n",
        "        self.discrete = discrete\n",
        "        self.metric = metric\n",
        "        self.best_thr = 0.5\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.name)\n",
        "\n",
        "    def __call__(self, y_true, y_pred, thr=0.5, use_best=False):\n",
        "        if self.discrete:\n",
        "            return self.metric(y_true, y_pred, thr=thr if not use_best else self.best_thr)\n",
        "        else:\n",
        "            return self.metric(y_true, y_pred)\n",
        "\n",
        "    def find_threshold(self, y_true, y_pred):\n",
        "        if self.optimize:\n",
        "            w0 = [0.5]\n",
        "            res = fmin(self.opt, w0, args=(y_true, y_pred), disp=0)[0]\n",
        "            self.best_thr = res\n",
        "            return res\n",
        "        else:\n",
        "            return 0.5\n",
        "\n",
        "    def opt(self, w, y_true, y_pred):\n",
        "        return (-1) ** (self.higher_is_better) * self(y_true, y_pred, w[0])\n",
        "\n",
        "\n",
        "def f1_custom(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='micro')\n",
        "\n",
        "\n",
        "def f1_macro(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='macro')\n",
        "\n",
        "\n",
        "def acc_score(y_true, y_pred, thr=0.5):\n",
        "    return accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "def bacc_score(y_true, y_pred, thr=0.5):\n",
        "    return balanced_accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "class MetricFactory:\n",
        "    def __init__(self, ):\n",
        "        self.metrics = {\n",
        "            'auc': Metric(metric=roc_auc_score, higher_is_better=True, name='auc', optimize=False, discrete=False),\n",
        "            'log-loss': Metric(metric=log_loss, higher_is_better=False, name='log-loss', optimize=False,\n",
        "                              discrete=False),\n",
        "            'f1': Metric(metric=f1_custom, higher_is_better=True, name='f1', optimize=True, discrete=True),\n",
        "            'f1-macro': Metric(metric=f1_macro, higher_is_better=True, name='f1_macro', optimize=True, discrete=True),\n",
        "            'balanced-acc': Metric(metric=bacc_score, higher_is_better=True, name='balanced-acc', optimize=True,\n",
        "                                   discrete=True),\n",
        "            'acc': Metric(metric=acc_score, higher_is_better=True, name='acc', optimize=True, discrete=True),\n",
        "            'mse': Metric(metric=mean_squared_error, higher_is_better=False, name='mse', optimize=False, discrete=False),\n",
        "            'r2': Metric(metric=r2_score, higher_is_better=True, name='r2', optimize=False, discrete=False),\n",
        "            'mae': Metric(metric=mean_absolute_error, higher_is_better=False, name='mae', optimize=False, discrete=False)\n",
        "        }\n",
        "\n",
        "    def get_allowed(self):\n",
        "        return sorted(list(self.metrics.keys()))\n",
        "\n",
        "    def add(self, metric_name, metric_class):\n",
        "        self.metrics[metric_name] = metric_class\n",
        "        return self\n",
        "\n",
        "    def remove(self, metric_name):\n",
        "        del self.models[metric_name]\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self, metric_name):\n",
        "        return deepcopy(self.metrics[metric_name])\n"
      ],
      "metadata": {
        "id": "cSppCKhPVeTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluation"
      ],
      "metadata": {
        "id": "jKTbM1dWVh85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(\n",
        "    outputs,\n",
        "    targets,\n",
        ") -> Dict[str, float]:\n",
        "    metrics = {}\n",
        "    \n",
        "    y_true = np.array(targets.cpu())\n",
        "    y_pred = (np.array(outputs.detach().cpu()))\n",
        "    \n",
        "    mse = metric_factory['mse']\n",
        "    mse_score = mse(y_true, y_pred)\n",
        "    \n",
        "    mae = metric_factory['mae']\n",
        "    mae_score = mae(y_true, y_pred)\n",
        "    \n",
        "    r2 = metric_factory['r2']\n",
        "    r2_score = r2(y_true, y_pred)\n",
        "    \n",
        "    metrics['rmse'] = np.sqrt(mse_score)\n",
        "    metrics['mae'] = mae_score\n",
        "    metrics['r2'] = r2_score\n",
        "    \n",
        "    return metrics"
      ],
      "metadata": {
        "id": "y8FQPWD6VgG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ftt(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    silent: bool\n",
        ") -> None:\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "    if not silent:\n",
        "        for i, (data, targets) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "        ):\n",
        "\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            data_num = data\n",
        "            data_cat = None\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data_num, data_cat)\n",
        "            loss = criterion(pred, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    else:\n",
        "        for i, (data, targets) in enumerate(dataloader):\n",
        "\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            data_num = data\n",
        "            data_cat = None\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data_num, data_cat)\n",
        "            loss = criterion(pred, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "Iq2yhDr2VkV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_epoch_ftt(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    writer: list,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    dataset: str,\n",
        "    glob_silent: bool,\n",
        "    silent: bool\n",
        ") -> None:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "    \n",
        "    true_val = None\n",
        "    pred_val = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if dataset == 'train':\n",
        "            desc = 'loop over train batches'\n",
        "        else:\n",
        "            desc = 'loop over test batches'\n",
        "\n",
        "        if not glob_silent:\n",
        "            for i, (data, targets) in tqdm(\n",
        "                enumerate(dataloader),\n",
        "                total=len(dataloader),\n",
        "                desc=desc,\n",
        "            ):\n",
        "\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                data_num = data\n",
        "                data_cat = None\n",
        "                outputs = model(data_num, data_cat)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                epoch_loss.append(loss.item())\n",
        "\n",
        "                if true_val is None:\n",
        "                    true_val = targets\n",
        "                else:\n",
        "                    true_val = torch.cat((true_val, targets), 0)\n",
        "                \n",
        "                if pred_val is None:\n",
        "                    pred_val = outputs\n",
        "                else:\n",
        "                    pred_val = torch.cat((pred_val, outputs), 0)\n",
        "        else: \n",
        "            for i, (data, targets) in enumerate(dataloader):\n",
        "\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                data_num = data\n",
        "                data_cat = None\n",
        "                outputs = model(data_num, data_cat)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                epoch_loss.append(loss.item())\n",
        "\n",
        "                if true_val is None:\n",
        "                    true_val = targets\n",
        "                else:\n",
        "                    true_val = torch.cat((true_val, targets), 0)\n",
        "                \n",
        "                if pred_val is None:\n",
        "                    pred_val = outputs\n",
        "                else:\n",
        "                    pred_val = torch.cat((pred_val, outputs), 0)\n",
        "                    \n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs = pred_val,\n",
        "            targets = true_val\n",
        "        )\n",
        "        \n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "\n",
        "\n",
        "        loss = criterion(pred_val, true_val)\n",
        "        if dataset == 'test' and scheduler is not None:\n",
        "            scheduler.step(loss)\n",
        "        val_loss = loss.item()\n",
        "        \n",
        "        if not silent:\n",
        "            if dataset == 'train':\n",
        "                print(f'Train loss: {val_loss}\\n')\n",
        "            else:\n",
        "                print(f'Test loss: {val_loss}\\n')\n",
        "\n",
        "        writer.append((batch_metrics_list, val_loss))\n",
        "        \n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = metric_value_list[0]\n",
        "            if not silent:\n",
        "                if dataset == 'train':\n",
        "                    print(f'Train {metric_name}: {metric_value}\\n')\n",
        "                else:\n",
        "                    print(f'Test {metric_name}: {metric_value}\\n')"
      ],
      "metadata": {
        "id": "IlcHk8U4WOeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ftt(\n",
        "    n_epochs: int,\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    val_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer_train: list,\n",
        "    writer_val: list,\n",
        "    writer_test: list,\n",
        "    device: torch.device,\n",
        "    patience=10,\n",
        "    silent=False,\n",
        "    glob_silent=False\n",
        ") -> None:\n",
        "    best_epoch = -1\n",
        "    best_metric = np.inf\n",
        "    best_r2 = np.inf\n",
        "    best_mae = np.inf\n",
        "    \n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "\n",
        "        train_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            silent=silent\n",
        "        )\n",
        "        evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_train,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            dataset='train',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "        )\n",
        "        evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=val_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_val,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            dataset='test',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "        )\n",
        "        temp_metric = writer_val[-1][0]['rmse'][0]\n",
        "        if temp_metric < best_metric:\n",
        "            best_epoch = epoch\n",
        "            best_metric = temp_metric\n",
        "            best_r2 = writer_val[-1][0]['r2'][0]\n",
        "            best_mae = writer_val[-1][0]['mae'][0]\n",
        "        elif epoch - best_epoch > patience:\n",
        "            break\n",
        "    evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_test,\n",
        "            device=device,\n",
        "            epoch=None,\n",
        "            dataset='test',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "    )\n",
        "    rmse = writer_test[-1][0]['rmse'][0]\n",
        "    r2 = writer_test[-1][0]['r2'][0]\n",
        "    mae = writer_test[-1][0]['mae'][0]\n",
        "    return best_metric, best_r2, best_mae\n",
        "    "
      ],
      "metadata": {
        "id": "mClrZOGFWTAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "w_qSD2nOWda3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NYC Taxi"
      ],
      "metadata": {
        "id": "p6-9S35_WhsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "df_nt = fetch_openml(data_id=42729, as_frame=True, parser='auto').frame\n",
        "df_nt = df_nt[df_nt['tip_amount'] <= 20]\n",
        "nf_nt = ['PULocationID', 'DOLocationID', 'passenger_count', 'tolls_amount', 'total_amount',\n",
        "         'lpep_pickup_datetime_day', 'lpep_pickup_datetime_hour', 'lpep_pickup_datetime_minute',\n",
        "        'lpep_dropoff_datetime_day', 'lpep_dropoff_datetime_hour', 'lpep_dropoff_datetime_minute']\n",
        "cf_nt = ['VendorID', 'store_and_fwd_flag', 'RatecodeID', 'extra', 'mta_tax', \n",
        "        'improvement_surcharge', 'trip_type']\n",
        "scaler = StandardScaler()\n",
        "df_nt[nf_nt] = scaler.fit_transform(df_nt[nf_nt])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_nt[cf_nt])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_nt.index\n",
        "df_nt = df_nt.drop(cf_nt, axis=1)\n",
        "df_nt = pd.concat([df_nt, tdf], axis=1)\n",
        "df_nt_target_name = 'tip_amount'\n",
        "df_nt.shape"
      ],
      "metadata": {
        "id": "pgX1xslQWf81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colleges"
      ],
      "metadata": {
        "id": "b3L31h6OWkNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cl = fetch_openml(data_id=42727, as_frame=True).frame\n",
        "df_cl.fillna(df_cl.median(), inplace=True)\n",
        "nf_cl = ['city', 'state', 'zip', 'latitude', 'longitude', 'admission_rate',\n",
        "        'sat_verbal_midrange', 'sat_math_midrange', 'sat_writing_midrange', 'act_combined_midrange', \n",
        "        'act_english_midrange', 'act_math_midrange', 'act_writing_midrange', 'sat_total_average', 'undergrad_size',\n",
        "        'percent_white', 'percent_black', 'percent_hispanic', 'percent_asian', 'percent_part_time', \n",
        "        'average_cost_academic_year', 'average_cost_program_year', 'tuition_(instate)', 'tuition_(out_of_state)',\n",
        "        'spend_per_student', 'faculty_salary', 'percent_part_time_faculty', 'completion_rate', 'percent_female',\n",
        "        'agege24', 'faminc', 'mean_earnings_6_years', 'median_earnings_6_years', 'mean_earnings_10_years',\n",
        "        'median_earnings_10_years', 'carnegie_basic_classification', 'carnegie_undergraduate', 'carnegie_size',\n",
        "        'religious_affiliation', ]\n",
        "cf_cl = ['predominant_degree', 'highest_degree', 'ownership', 'region', 'gender']\n",
        "le = LabelEncoder()\n",
        "df_cl['city'] = le.fit_transform(df_cl['city'])\n",
        "df_cl['state'] = le.fit_transform(df_cl['state'])\n",
        "df_cl['zip'] = le.fit_transform(df_cl['zip'])\n",
        "df_cl['carnegie_basic_classification'] = le.fit_transform(df_cl['carnegie_basic_classification'])\n",
        "df_cl['carnegie_undergraduate'] = le.fit_transform(df_cl['carnegie_undergraduate'])\n",
        "df_cl['carnegie_size'] = le.fit_transform(df_cl['carnegie_size'])\n",
        "df_cl['religious_affiliation'] = le.fit_transform(df_cl['religious_affiliation'])\n",
        "scaler = StandardScaler()\n",
        "df_cl[nf_cl] = scaler.fit_transform(df_cl[nf_cl])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_cl[cf_cl])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_cl.index\n",
        "df_cl = df_cl.drop(cf_cl, axis=1)\n",
        "df_cl = pd.concat([df_cl, tdf], axis=1)\n",
        "df_cl_target_name = 'percent_pell_grant'\n",
        "df_cl.shape"
      ],
      "metadata": {
        "id": "RUOM5nidWlse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### House sales"
      ],
      "metadata": {
        "id": "pLqfbGLQWnlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hs = fetch_openml(data_id=42731, as_frame=True).frame\n",
        "df_hs.fillna(df_hs.median(), inplace=True)\n",
        "df_hs = df_hs[df_hs['price'] <= 3000000]\n",
        "nf_hs = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', \n",
        "    'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'date_year', 'date_month', 'date_day']\n",
        "cf_hs = ['floors', 'waterfront', 'view', 'condition', 'grade']\n",
        "le = LabelEncoder()\n",
        "df_hs['zipcode'] = le.fit_transform(df_hs['zipcode'])\n",
        "scaler = StandardScaler()\n",
        "df_hs[nf_hs] = scaler.fit_transform(df_hs[nf_hs])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_hs[cf_hs])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_hs.index\n",
        "df_hs = df_hs.drop(cf_hs, axis=1)\n",
        "df_hs = pd.concat([df_hs, tdf], axis=1)\n",
        "df_hs_target_name = 'price'\n",
        "df_hs.shape"
      ],
      "metadata": {
        "id": "hZxZxuIwWpkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Black friday"
      ],
      "metadata": {
        "id": "lR62bmTHWrUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bf = fetch_openml(data_id=41540, as_frame=True).frame\n",
        "df_bf.fillna(df_bf.median(), inplace=True)\n",
        "nf_bf = ['Occupation', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
        "cf_bf = ['Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status']\n",
        "le = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "df_bf[nf_bf] = scaler.fit_transform(df_bf[nf_bf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bf[cf_bf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bf.index\n",
        "df_bf = df_bf.drop(cf_bf, axis=1)\n",
        "df_bf = pd.concat([df_bf, tdf], axis=1)\n",
        "df_bf_target_name = 'Purchase'\n",
        "df_bf.shape"
      ],
      "metadata": {
        "id": "VUunq2R4Ws2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beijing PM2.5"
      ],
      "metadata": {
        "id": "LVAC7afxWw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = pd.read_csv('./PRSA_data_2010.1.1-2014.12.31.csv')\n",
        "df_bp.drop(['No'], axis=1, inplace=True)\n",
        "df_bp.fillna(df_bp.median(), inplace=True)\n",
        "df_bp = df_bp[df_bp['pm2.5'] <= 600]\n",
        "nf_bp = ['month', 'day', 'hour', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "cf_bp = ['year', 'cbwd']\n",
        "le = LabelEncoder()\n",
        "df_bp['year'] = le.fit_transform(df_bp['year'])\n",
        "df_bp['cbwd'] = le.fit_transform(df_bp['cbwd'])\n",
        "scaler = StandardScaler()\n",
        "df_bp[nf_bp] = scaler.fit_transform(df_bp[nf_bp])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bp[cf_bp])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bp.index\n",
        "df_bp = df_bp.drop(cf_bp, axis=1)\n",
        "df_bp = pd.concat([df_bp, tdf], axis=1)\n",
        "df_bp_target_name = 'pm2.5'\n",
        "df_bp.shape"
      ],
      "metadata": {
        "id": "4OAHfMzsWvJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brazilian houses"
      ],
      "metadata": {
        "id": "1Onbr6wGW0jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bh = fetch_openml(data_id=42688, as_frame=True).frame\n",
        "df_bh.fillna(df_bh.median(), inplace=True)\n",
        "df_bh = df_bh[df_bh['total_(BRL)'] <= 40000]\n",
        "nf_bh = ['area', 'rooms', 'bathroom', 'parking_spaces', 'floor', 'hoa_(BRL)', 'rent_amount_(BRL)',\n",
        "        'property_tax_(BRL)', 'fire_insurance_(BRL)']\n",
        "cf_bh = ['city', 'animal', 'furniture']\n",
        "le = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "df_bh['city'] = le.fit_transform(df_bh['city'])\n",
        "df_bh['animal'] = le.fit_transform(df_bh['animal'])\n",
        "df_bh['furniture'] = le.fit_transform(df_bh['furniture'])\n",
        "df_bh[nf_bh] = scaler.fit_transform(df_bh[nf_bh])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bh[cf_bh])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bh.index\n",
        "df_bh = df_bh.drop(cf_bh, axis=1)\n",
        "df_bh = pd.concat([df_bh, tdf], axis=1)\n",
        "df_bh_target_name = 'total_(BRL)'\n",
        "df_bh.shape"
      ],
      "metadata": {
        "id": "xsHUfer1W1Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runs"
      ],
      "metadata": {
        "id": "RGsycgkUW4Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {'nt': df_nt, 'cl': df_cl, 'hs': df_hs, 'bf': df_bf, 'bp': df_bp, 'bh': df_bh}\n",
        "dfs_shapes = {'nt': df_nt.shape[1]-1, 'cl': df_cl.shape[1]-1, 'hs': df_hs.shape[1]-1, 'bf': df_bf.shape[1]-1, 'bp': df_bp.shape[1]-1, 'bh': df_bh.shape[1]-1}\n",
        "dfs_targets = {'nt': df_nt_target_name, 'cl': df_cl_target_name, 'hs': df_hs_target_name, 'bf': df_bf_target_name, 'bp': df_bp_target_name, 'bh': df_bh_target_name}\n",
        "dfs_names = ['nt', 'cl', 'hs', 'bf', 'bp', 'bh']\n",
        "preproc_types = ['Periodic', 'Fourier', 'Linear', 'AutoDis', 'Tokens', 'SoftEmbedding']"
      ],
      "metadata": {
        "id": "MWCxHljwW40w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_nt.shape)\n",
        "print(df_cl.shape)\n",
        "print(df_hs.shape)\n",
        "print(df_bf.shape)\n",
        "print(df_bp.shape)\n",
        "print(df_bh.shape)"
      ],
      "metadata": {
        "id": "QnsjbB4HW6ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preproc_params(dataset, preproc_type, tf, d_embedding):\n",
        "    preproc_params = {}\n",
        "    if preproc_type == 'Periodic':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'n': d_embedding, 'sigma': 0.1, 'trainable': True,\n",
        "                         'initialization': 'normal', 'tf': tf}\n",
        "        preproc_name = 'Periodic'\n",
        "    elif preproc_type == 'Fourier':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'n': d_embedding, 'sigma': 0.1, 'trainable': False,\n",
        "                         'initialization': 'normal', 'tf': tf}\n",
        "        preproc_name = 'Periodic'\n",
        "    elif preproc_type == 'Linear':\n",
        "        preproc_params = {'n_layers': 2, 'n_features': dfs_shapes[dataset], \n",
        "                          'd_embeddings': [d_embedding, d_embedding], 'tf': tf}\n",
        "        preproc_name = 'Linear'\n",
        "    elif preproc_type == 'AutoDis':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'd_embedding': d_embedding, 'n_meta_embeddings': 20,\n",
        "                         'temperature': 0.5, 'tf': tf}\n",
        "        preproc_name = 'AutoDis'\n",
        "    elif preproc_type == 'Tokens' or preproc_type == 'ARM':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'd_embedding': d_embedding, 'tf': tf}\n",
        "        preproc_name = preproc_type\n",
        "    elif preproc_type == 'SoftEmbedding':\n",
        "        preproc_params = {'num_embeddings': dfs_shapes[dataset], 'embeddings_dim': d_embedding, \n",
        "                          'emb_initializer': None, 'tf': tf}\n",
        "        preproc_name = 'SoftEmbedding'\n",
        "    elif preproc_type == 'BinEncoding' or preproc_type == 'ARM_Bin':\n",
        "        preproc_params = {'bin_edges': bin_edges, 'bins': bins, 'bin_values': bin_values, 'nbins': nbins,\n",
        "                         'tf': tf, 'd_token': d_embedding, 'bias': True, 'initialization': 'normal', \n",
        "                          'device': device}\n",
        "        preproc_name = 'BinEncoding'\n",
        "    return preproc_params, preproc_name"
      ],
      "metadata": {
        "id": "zf6FvI44W88k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './output/reg/mlp/'\n",
        "positional = False\n",
        "tf = False\n",
        "d_embedding = 10"
      ],
      "metadata": {
        "id": "rrRGGnrQW-we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "1JnzCvlIXAXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for df_name in tqdm(['cl']):\n",
        "    for preproc_type in tqdm(preproc_types):\n",
        "        temp_path = df_name + '/' + preproc_type + '/'\n",
        "        X = dfs[df_name].drop([dfs_targets[df_name]], axis=1)\n",
        "        y = dfs[df_name][dfs_targets[df_name]]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "        X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
        "        temp = []\n",
        "        final_res = defaultdict(list)\n",
        "        n_runs=15\n",
        "        silent=True,\n",
        "        glob_silent=True\n",
        "        best_metric = np.inf\n",
        "        best_r2 = np.inf\n",
        "        best_mae = np.inf\n",
        "        \n",
        "        cat_len = 0\n",
        "        seeds = np.random.randint(1, 100500, 15)\n",
        "        for i in range(n_runs):\n",
        "            if not silent:\n",
        "                print(f'Run {i+1}:')\n",
        "                print()\n",
        "            set_global_seed(seeds[i])\n",
        "            res_metrics = defaultdict(list)   \n",
        "            X_train_t = torch.from_numpy(X_train_t.values).float()\n",
        "            y_train_t = torch.from_numpy(y_train_t.values.ravel()).float().unsqueeze(1)\n",
        "            X_test_t = torch.from_numpy(X_test_t.values).float()\n",
        "            y_test_t = torch.from_numpy(y_test_t.values.ravel()).float().unsqueeze(1)\n",
        "            X_test = torch.from_numpy(X_test.values).float()\n",
        "            y_test = torch.from_numpy(y_test.values.ravel()).float().unsqueeze(1)\n",
        "        \n",
        "            train_ds = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "            val_ds = torch.utils.data.TensorDataset(X_test_t, y_test_t)\n",
        "            test_ds = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "            train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "            val_dataloader = torch.utils.data.DataLoader(val_ds, batch_size=256, shuffle=True)\n",
        "            test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=256, shuffle=True)\n",
        "    \n",
        "            writer_train = []\n",
        "            writer_val = []\n",
        "            writer_test = []\n",
        "      \n",
        "        \n",
        "            metric_factory = MetricFactory()\n",
        "            preproc_params, preproc_name = get_preproc_params(df_name, preproc_type, tf, d_embedding)\n",
        "                \n",
        "                \n",
        "            preproc_params['tf'] = True\n",
        "            feature_tokenizer = FeatureTokenizer(X.shape[1], None, d_embedding, \n",
        "                                                     preproc_name, 'Lookup', preproc_params, positional, tf)\n",
        "            model = MLP(n_in=dfs_shapes[df_name], hidden_size=[512, 256, 128], drop_rate=[0.1, 0.1, 0.1], \n",
        "                device=device, use_bn=True, use_noise=True, use_dropout=True,\n",
        "                preproc=False, preproc_type=preproc_name, preproc_args=preproc_params,\n",
        "                        positional=positional, tokenizer=feature_tokenizer).to(device)\n",
        "\n",
        "                \n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=0)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, patience=5, factor=1e-1, min_lr=1e-6)\n",
        "            criterion = torch.nn.MSELoss()\n",
        "                \n",
        "            metric_rmse, metric_r2, metric_mae = train_ftt(100, model, train_dataloader, val_dataloader, test_dataloader, optimizer, None, criterion, writer_train,\n",
        "                           writer_val, writer_test, device, 20, silent, glob_silent)\n",
        "            test_metrics = writer_test[-1][0]\n",
        "            test_loss = writer_test[-1][1]\n",
        "            for key, value in test_metrics.items():\n",
        "                res_metrics[key].append(value[0])\n",
        "            res_metrics['loss'].append(test_loss)\n",
        "            for key, value in res_metrics.items():\n",
        "                final_res[key].append(np.mean(value))\n",
        "            s = 'run' + str(i + 1) + '.pickle'\n",
        "            with open(path + temp_path + s, 'wb') as f:\n",
        "                pickle.dump(res_metrics, f)\n",
        "    \n",
        "        ans = {}\n",
        "        for key, value in final_res.items():\n",
        "            ans[key] = float(np.mean(value))\n",
        "\n",
        "        with open(path + temp_path + 'metrics.yml', 'w') as f:\n",
        "            ruamel.yaml.round_trip_dump(ans, f)"
      ],
      "metadata": {
        "id": "Yxa4GcuoXBzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}