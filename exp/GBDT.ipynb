{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEWsw8F72vjn"
      },
      "outputs": [],
      "source": [
        "!pip install optuna rtdl category_encoders ruamel.yaml einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import fmin\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pathlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from tqdm import tqdm, trange\n",
        "from typing import Optional, Sequence, Tuple, Union, Any, Dict, List\n",
        "from copy import deepcopy\n",
        "import enum\n",
        "import optuna\n",
        "import rtdl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import category_encoders as ce\n",
        "import ruamel.yaml\n",
        "import math\n",
        "from collections import OrderedDict, defaultdict\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from scipy.spatial import distance_matrix\n",
        "from scipy.linalg import qr\n",
        "from einops import rearrange\n",
        "from xgboost import XGBRegressor, XGBClassifier"
      ],
      "metadata": {
        "id": "yNM34fmq2-7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_global_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ],
      "metadata": {
        "id": "2SyvyWzm3IYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Q606vObB3LIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "nPND2M463NMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric():\n",
        "    def __init__(self, metric, higher_is_better=True, name='name', optimize=False, discrete=False, **kwargs):\n",
        "        self.name = name\n",
        "        self.higher_is_better = higher_is_better\n",
        "        self.optimize = optimize\n",
        "        self.discrete = discrete\n",
        "        self.metric = metric\n",
        "        self.best_thr = 0.5\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.name)\n",
        "\n",
        "    def __call__(self, y_true, y_pred, thr=0.5, use_best=False):\n",
        "        if self.discrete:\n",
        "            return self.metric(y_true, y_pred, thr=thr if not use_best else self.best_thr)\n",
        "        else:\n",
        "            return self.metric(y_true, y_pred)\n",
        "\n",
        "    def find_threshold(self, y_true, y_pred):\n",
        "        if self.optimize:\n",
        "            w0 = [0.5]\n",
        "            res = fmin(self.opt, w0, args=(y_true, y_pred), disp=0)[0]\n",
        "            self.best_thr = res\n",
        "            return res\n",
        "        else:\n",
        "            return 0.5\n",
        "\n",
        "    def opt(self, w, y_true, y_pred):\n",
        "        return (-1) ** (self.higher_is_better) * self(y_true, y_pred, w[0])\n",
        "\n",
        "\n",
        "def f1_custom(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='micro')\n",
        "\n",
        "\n",
        "def f1_macro(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='macro')\n",
        "\n",
        "\n",
        "def acc_score(y_true, y_pred, thr=0.5):\n",
        "    return accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "def bacc_score(y_true, y_pred, thr=0.5):\n",
        "    return balanced_accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "class MetricFactory:\n",
        "    def __init__(self, ):\n",
        "        self.metrics = {\n",
        "            'auc': Metric(metric=roc_auc_score, higher_is_better=True, name='auc', optimize=False, discrete=False),\n",
        "            'log-loss': Metric(metric=log_loss, higher_is_better=False, name='log-loss', optimize=False,\n",
        "                              discrete=False),\n",
        "            'f1': Metric(metric=f1_custom, higher_is_better=True, name='f1', optimize=True, discrete=True),\n",
        "            'f1-macro': Metric(metric=f1_macro, higher_is_better=True, name='f1_macro', optimize=True, discrete=True),\n",
        "            'balanced-acc': Metric(metric=bacc_score, higher_is_better=True, name='balanced-acc', optimize=True,\n",
        "                                   discrete=True),\n",
        "            'acc': Metric(metric=acc_score, higher_is_better=True, name='acc', optimize=True, discrete=True),\n",
        "            'mse': Metric(metric=mean_squared_error, higher_is_better=False, name='mse', optimize=False, discrete=False),\n",
        "            'r2': Metric(metric=r2_score, higher_is_better=True, name='r2', optimize=False, discrete=False),\n",
        "            'mae': Metric(metric=mean_absolute_error, higher_is_better=False, name='mae', optimize=False, discrete=False)\n",
        "        }\n",
        "\n",
        "    def get_allowed(self):\n",
        "        return sorted(list(self.metrics.keys()))\n",
        "\n",
        "    def add(self, metric_name, metric_class):\n",
        "        self.metrics[metric_name] = metric_class\n",
        "        return self\n",
        "\n",
        "    def remove(self, metric_name):\n",
        "        del self.models[metric_name]\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self, metric_name):\n",
        "        return deepcopy(self.metrics[metric_name])\n"
      ],
      "metadata": {
        "id": "D2KUrfP33PI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(\n",
        "    outputs,\n",
        "    targets,\n",
        ") -> Dict[str, float]:\n",
        "    metrics = {}\n",
        "    y_true = targets\n",
        "    y_pred = outputs\n",
        "\n",
        "    mse = metric_factory['mse']\n",
        "    mse_score = mse(y_true, y_pred)\n",
        "    \n",
        "    mae = metric_factory['mae']\n",
        "    mae_score = mae(y_true, y_pred)\n",
        "    \n",
        "    r2 = metric_factory['r2']\n",
        "    r2_score = r2(y_true, y_pred)\n",
        "    \n",
        "    metrics['rmse'] = np.sqrt(mse_score)\n",
        "    metrics['mae'] = mae_score\n",
        "    metrics['r2'] = r2_score\n",
        "    \n",
        "    return metrics"
      ],
      "metadata": {
        "id": "rCtb97bo3WJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "bTWQNRoR3idm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NYC Taxi"
      ],
      "metadata": {
        "id": "IyMXdBE33lWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "df_nt = fetch_openml(data_id=42729, as_frame=True, parser='auto').frame\n",
        "df_nt = df_nt[df_nt['tip_amount'] <= 20]\n",
        "nf_nt = ['PULocationID', 'DOLocationID', 'passenger_count', 'tolls_amount', 'total_amount',\n",
        "         'lpep_pickup_datetime_day', 'lpep_pickup_datetime_hour', 'lpep_pickup_datetime_minute',\n",
        "        'lpep_dropoff_datetime_day', 'lpep_dropoff_datetime_hour', 'lpep_dropoff_datetime_minute']\n",
        "cf_nt = ['VendorID', 'store_and_fwd_flag', 'RatecodeID', 'extra', 'mta_tax', \n",
        "        'improvement_surcharge', 'trip_type']\n",
        "scaler = StandardScaler()\n",
        "df_nt[nf_nt] = scaler.fit_transform(df_nt[nf_nt])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_nt[cf_nt])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_nt.index\n",
        "df_nt = df_nt.drop(cf_nt, axis=1)\n",
        "df_nt = pd.concat([df_nt, tdf], axis=1)\n",
        "df_nt_target_name = 'tip_amount'\n",
        "df_nt.shape"
      ],
      "metadata": {
        "id": "pbelyqcp3jBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colleges"
      ],
      "metadata": {
        "id": "mVQ0NKMa3uge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cl = fetch_openml(data_id=42727, as_frame=True).frame\n",
        "df_cl.fillna(df_cl.median(), inplace=True)\n",
        "nf_cl = ['city', 'state', 'zip', 'latitude', 'longitude', 'admission_rate',\n",
        "        'sat_verbal_midrange', 'sat_math_midrange', 'sat_writing_midrange', 'act_combined_midrange', \n",
        "        'act_english_midrange', 'act_math_midrange', 'act_writing_midrange', 'sat_total_average', 'undergrad_size',\n",
        "        'percent_white', 'percent_black', 'percent_hispanic', 'percent_asian', 'percent_part_time', \n",
        "        'average_cost_academic_year', 'average_cost_program_year', 'tuition_(instate)', 'tuition_(out_of_state)',\n",
        "        'spend_per_student', 'faculty_salary', 'percent_part_time_faculty', 'completion_rate', 'percent_female',\n",
        "        'agege24', 'faminc', 'mean_earnings_6_years', 'median_earnings_6_years', 'mean_earnings_10_years',\n",
        "        'median_earnings_10_years', 'carnegie_basic_classification', 'carnegie_undergraduate', 'carnegie_size',\n",
        "        'religious_affiliation', ]\n",
        "cf_cl = ['predominant_degree', 'highest_degree', 'ownership', 'region', 'gender']\n",
        "le = LabelEncoder()\n",
        "df_cl['city'] = le.fit_transform(df_cl['city'])\n",
        "df_cl['state'] = le.fit_transform(df_cl['state'])\n",
        "df_cl['zip'] = le.fit_transform(df_cl['zip'])\n",
        "df_cl['carnegie_basic_classification'] = le.fit_transform(df_cl['carnegie_basic_classification'])\n",
        "df_cl['carnegie_undergraduate'] = le.fit_transform(df_cl['carnegie_undergraduate'])\n",
        "df_cl['carnegie_size'] = le.fit_transform(df_cl['carnegie_size'])\n",
        "df_cl['religious_affiliation'] = le.fit_transform(df_cl['religious_affiliation'])\n",
        "scaler = StandardScaler()\n",
        "df_cl[nf_cl] = scaler.fit_transform(df_cl[nf_cl])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_cl[cf_cl])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_cl.index\n",
        "df_cl = df_cl.drop(cf_cl, axis=1)\n",
        "df_cl = pd.concat([df_cl, tdf], axis=1)\n",
        "df_cl_target_name = 'percent_pell_grant'\n",
        "df_cl.shape"
      ],
      "metadata": {
        "id": "RFt4smiY3rr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### House sales"
      ],
      "metadata": {
        "id": "M5cLo1ms3xhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hs = fetch_openml(data_id=42731, as_frame=True).frame\n",
        "df_hs.fillna(df_hs.median(), inplace=True)\n",
        "df_hs = df_hs[df_hs['price'] <= 3000000]\n",
        "nf_hs = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'yr_built', \n",
        "    'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15', 'date_year', 'date_month', 'date_day']\n",
        "cf_hs = ['floors', 'waterfront', 'view', 'condition', 'grade']\n",
        "le = LabelEncoder()\n",
        "df_hs['zipcode'] = le.fit_transform(df_hs['zipcode'])\n",
        "scaler = StandardScaler()\n",
        "df_hs[nf_hs] = scaler.fit_transform(df_hs[nf_hs])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_hs[cf_hs])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_hs.index\n",
        "df_hs = df_hs.drop(cf_hs, axis=1)\n",
        "df_hs = pd.concat([df_hs, tdf], axis=1)\n",
        "df_hs_target_name = 'price'\n",
        "df_hs.shape"
      ],
      "metadata": {
        "id": "TRU9UfmY33EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Black friday"
      ],
      "metadata": {
        "id": "9yVdLFOp37FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bf = fetch_openml(data_id=41540, as_frame=True).frame\n",
        "df_bf.fillna(df_bf.median(), inplace=True)\n",
        "nf_bf = ['Occupation', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3']\n",
        "cf_bf = ['Gender', 'Age', 'City_Category', 'Stay_In_Current_City_Years', 'Marital_Status']\n",
        "le = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "df_bf[nf_bf] = scaler.fit_transform(df_bf[nf_bf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bf[cf_bf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bf.index\n",
        "df_bf = df_bf.drop(cf_bf, axis=1)\n",
        "df_bf = pd.concat([df_bf, tdf], axis=1)\n",
        "df_bf_target_name = 'Purchase'\n",
        "df_bf.shape"
      ],
      "metadata": {
        "id": "osyJ0qpl35Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beijing PM2.5"
      ],
      "metadata": {
        "id": "XptH6Pm64CJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bp = pd.read_csv('./data/PRSA_data_2010.1.1-2014.12.31.csv')\n",
        "df_bp.drop(['No'], axis=1, inplace=True)\n",
        "df_bp.fillna(df_bp.median(), inplace=True)\n",
        "df_bp = df_bp[df_bp['pm2.5'] <= 600]\n",
        "nf_bp = ['month', 'day', 'hour', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']\n",
        "cf_bp = ['year', 'cbwd']\n",
        "le = LabelEncoder()\n",
        "df_bp['year'] = le.fit_transform(df_bp['year'])\n",
        "df_bp['cbwd'] = le.fit_transform(df_bp['cbwd'])\n",
        "scaler = StandardScaler()\n",
        "df_bp[nf_bp] = scaler.fit_transform(df_bp[nf_bp])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bp[cf_bp])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bp.index\n",
        "df_bp = df_bp.drop(cf_bp, axis=1)\n",
        "df_bp = pd.concat([df_bp, tdf], axis=1)\n",
        "df_bp_target_name = 'pm2.5'\n",
        "df_bp.shape"
      ],
      "metadata": {
        "id": "nZhem8kL3_8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brazilian houses"
      ],
      "metadata": {
        "id": "7NJH-Hhj4HOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_bh = fetch_openml(data_id=42688, as_frame=True).frame\n",
        "df_bh.fillna(df_bh.median(), inplace=True)\n",
        "df_bh = df_bh[df_bh['total_(BRL)'] <= 40000]\n",
        "nf_bh = ['area', 'rooms', 'bathroom', 'parking_spaces', 'floor', 'hoa_(BRL)', 'rent_amount_(BRL)',\n",
        "        'property_tax_(BRL)', 'fire_insurance_(BRL)']\n",
        "cf_bh = ['city', 'animal', 'furniture']\n",
        "le = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "df_bh['city'] = le.fit_transform(df_bh['city'])\n",
        "df_bh['animal'] = le.fit_transform(df_bh['animal'])\n",
        "df_bh['furniture'] = le.fit_transform(df_bh['furniture'])\n",
        "df_bh[nf_bh] = scaler.fit_transform(df_bh[nf_bh])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "transf = ohe.fit_transform(df_bh[cf_bh])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names_out())\n",
        "tdf.index = df_bh.index\n",
        "df_bh = df_bh.drop(cf_bh, axis=1)\n",
        "df_bh = pd.concat([df_bh, tdf], axis=1)\n",
        "df_bh_target_name = 'total_(BRL)'\n",
        "df_bh.shape"
      ],
      "metadata": {
        "id": "sTHjrnoO4Epf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Give Me Some Credit"
      ],
      "metadata": {
        "id": "hREavFlTnMh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsc = pd.read_csv('./data/gsc-training.csv', )\n",
        "df_gsc = df_gsc.drop(['Unnamed: 0'], axis=1)\n",
        "df_gsc.fillna(df_gsc.median(), inplace=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "nf_gsc = ['RevolvingUtilizationOfUnsecuredLines', 'DebtRatio', 'MonthlyIncome', 'age']\n",
        "cf_gsc = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents']\n",
        "df_gsc[nf_gsc] = scaler.fit_transform(df_gsc[nf_gsc])\n",
        "le = LabelEncoder()\n",
        "cc_gsc = []\n",
        "for cf_name in cf_gsc:\n",
        "    df_gsc[cf_name] = le.fit_transform(df_gsc[cf_name])\n",
        "    cc_gsc.append(len(np.unique(df_gsc[cf_name])))\n",
        "tdf = df_gsc[cf_gsc]\n",
        "df_gsc = df_gsc.drop(cf_gsc, axis=1)\n",
        "df_gsc = pd.concat([df_gsc, tdf], axis=1)\n",
        "df_gsc_target_name = 'SeriousDlqin2yrs'\n",
        "df_gsc.shape"
      ],
      "metadata": {
        "id": "DP52iQ5uBXJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Churn Modelling"
      ],
      "metadata": {
        "id": "QYJf1wFtnTb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.read_csv('./data/Churn_Modelling.csv')\n",
        "df_cm = df_cm.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
        "df_cm.fillna(df_cm.median(), inplace=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "nf_cm = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']\n",
        "cf_cm = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n",
        "df_cm[nf_cm] = scaler.fit_transform(df_cm[nf_cm])\n",
        "le = LabelEncoder()\n",
        "cc_cm = []\n",
        "for cf_name in cf_cm:\n",
        "    df_cm[cf_name] = le.fit_transform(df_cm[cf_name])\n",
        "    cc_cm.append(len(np.unique(df_cm[cf_name])))\n",
        "tdf = df_cm[cf_cm]\n",
        "df_cm = df_cm.drop(cf_cm, axis=1)\n",
        "df_cm = pd.concat([df_cm, tdf], axis=1)\n",
        "df_cm_target_name = 'Exited'\n",
        "df_cm.shape"
      ],
      "metadata": {
        "id": "MyB5Oakmotmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vehicle Loan Default"
      ],
      "metadata": {
        "id": "4G7V0xoDngdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vld = pd.read_csv('./data/vehicle_loan_default_train.csv')\n",
        "df_vld = df_vld.drop(['UNIQUEID', 'EMPLOYEE_CODE_ID', 'MOBILENO_AVL_FLAG'], axis=1)\n",
        "df_vld.fillna(df_vld.median(), inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_vld['AVERAGE_ACCT_AGE'] = le.fit_transform(df_vld['AVERAGE_ACCT_AGE'])\n",
        "df_vld['CREDIT_HISTORY_LENGTH'] = le.fit_transform(df_vld['CREDIT_HISTORY_LENGTH'])\n",
        "\n",
        "\n",
        "df_vld['DATE_OF_BIRTH'] = pd.to_datetime(df_vld['DATE_OF_BIRTH'], format='%d-%m-%Y')\n",
        "df_vld['DATE_OF_BIRTH_d'] = df_vld['DATE_OF_BIRTH'].dt.day\n",
        "df_vld['DATE_OF_BIRTH_m'] = df_vld['DATE_OF_BIRTH'].dt.month\n",
        "df_vld['DATE_OF_BIRTH_y'] = df_vld['DATE_OF_BIRTH'].dt.year\n",
        "df_vld = df_vld.drop(['DATE_OF_BIRTH'], axis=1)\n",
        "df_vld['DISBURSAL_DATE'] = pd.to_datetime(df_vld['DISBURSAL_DATE'], format='%d-%m-%Y')\n",
        "df_vld['DISBURSAL_DATE_d'] = df_vld['DISBURSAL_DATE'].dt.day\n",
        "df_vld['DISBURSAL_DATE_m'] = df_vld['DISBURSAL_DATE'].dt.month\n",
        "df_vld['DISBURSAL_DATE_y'] = df_vld['DISBURSAL_DATE'].dt.year\n",
        "df_vld = df_vld.drop(['DISBURSAL_DATE'], axis=1)\n",
        "\n",
        "\n",
        "nf_vld = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'BRANCH_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', \n",
        "     'PERFORM_CNS_SCORE', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT', 'SEC_CURRENT_BALANCE',\n",
        "     'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NO_OF_INQUIRIES', \n",
        "      'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', \n",
        "      'SEC_OVERDUE_ACCTS', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n",
        "     'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'DATE_OF_BIRTH_d', 'DATE_OF_BIRTH_m', 'DATE_OF_BIRTH_y', \n",
        "     'DISBURSAL_DATE_d', 'DISBURSAL_DATE_m', 'DISBURSAL_DATE_y'] \n",
        "cf_vld = ['EMPLOYMENT_TYPE', 'STATE_ID', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG',\n",
        "     'PASSPORT_FLAG', 'PERFORM_CNS_SCORE_DESCRIPTION']\n",
        "scaler = StandardScaler()\n",
        "df_vld[nf_vld] = scaler.fit_transform(df_vld[nf_vld])\n",
        "\n",
        "cc_vld = []\n",
        "for cf_name in cf_vld:\n",
        "    df_vld[cf_name] = le.fit_transform(df_vld[cf_name])\n",
        "    cc_vld.append(len(np.unique(df_vld[cf_name])))\n",
        "\n",
        "tdf = df_vld[cf_vld]\n",
        "df_vld = df_vld.drop(cf_vld, axis=1)\n",
        "df_vld = pd.concat([df_vld, tdf], axis=1)\n",
        "df_vld_target_name = 'LOAN_DEFAULT'\n",
        "df_vld.shape"
      ],
      "metadata": {
        "id": "zJTimanAO_pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adult Income Dataset"
      ],
      "metadata": {
        "id": "4lQqr7sKo1Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ai = pd.read_csv('./data/adult.csv')\n",
        "df_ai.replace('<=50K', 0, inplace=True)\n",
        "df_ai.replace('>50K', 1, inplace=True)\n",
        "df_ai.fillna(df_ai.median(), inplace=True)\n",
        "nf_ai = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week'] \n",
        "cf_ai = ['workclass', 'education', 'educational-num', 'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "     'native-country']\n",
        "scaler = StandardScaler()\n",
        "df_ai[nf_ai] = scaler.fit_transform(df_ai[nf_ai])\n",
        "\n",
        "cc_ai = []\n",
        "for cf_name in cf_ai:\n",
        "    df_ai[cf_name] = le.fit_transform(df_ai[cf_name])\n",
        "    cc_ai.append(len(np.unique(df_ai[cf_name])))\n",
        "\n",
        "tdf = df_ai[cf_ai]\n",
        "tdf.index = df_ai.index\n",
        "df_ai = df_ai.drop(cf_ai, axis=1)\n",
        "df_ai = pd.concat([df_ai, tdf], axis=1)\n",
        "df_ai_target_name = 'income'\n",
        "df_ai.shape"
      ],
      "metadata": {
        "id": "JYLMnBf-o8rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HELOC"
      ],
      "metadata": {
        "id": "rk1E4SJXo_Q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_heloc = pd.read_csv('./data/RiskData.csv')\n",
        "df_heloc.replace('.', 0, inplace=True)\n",
        "df_heloc = df_heloc.drop(['Sampling_Weight'], axis=1)\n",
        "\n",
        "nf_heloc = ['Interest_Revenue', 'Application_Date', 'Age', 'Income', 'Debt_Ratio', 'Loan_Amt',\n",
        "     'Loan_Amt_Req', 'LTV', 'FICO_Score', 'Prior_Custom_Score', 'Current_Custom_Score', 'CB_Age_Oldest_TL',\n",
        "     'CB_Age_Newest_TL', 'CB_Avg_Mos_File', 'CB_Nb_Sat_TL', 'CB_Pct_Sat_TL', 'CB_Mos_Since_Dlq', 'CB_Max_Dlq_12_Mos',\n",
        "     'CB_Max_Dlq_Ever', 'CB_Nb_Total_TL', 'CB_Nb_TL_Open_12', 'CB_Pct_IL_TL', 'CB_Nb_Inq_6_Mos', \n",
        "     'CB_Nb_Inq_6_Mos_excl_7_Days', 'CB_Rev_Util', 'CB_IL_Util', 'CB_Nb_Rev_TL_w_Bal', 'CB_Nb_IL_TL_w_Bal', \n",
        "     'CB_Nb_Rev_Tl_75_Pct_Limit', 'CB_Pct_TL_w_Bal']\n",
        "cf_heloc = ['Nb_Borrowers', 'Region', 'Bank_Relationship', 'CB_Nb_60_Plus_TL', 'CB_Nb_90_Plus_TL']\n",
        "scaler = StandardScaler()\n",
        "df_heloc.fillna(df_heloc.median(), inplace=True)\n",
        "df_heloc[nf_heloc] = df_heloc[nf_heloc].astype(float)\n",
        "df_heloc[cf_heloc] = df_heloc[cf_heloc].astype(str)\n",
        "\n",
        "df_heloc[nf_heloc] = scaler.fit_transform(df_heloc[nf_heloc])\n",
        "\n",
        "cc_heloc = []\n",
        "for cf_name in cf_heloc:\n",
        "    df_heloc[cf_name] = le.fit_transform(df_heloc[cf_name])\n",
        "    cc_heloc.append(len(np.unique(df_heloc[cf_name])))\n",
        "\n",
        "tdf = df_heloc[cf_heloc]\n",
        "df_heloc = df_heloc.drop(cf_heloc, axis=1)\n",
        "df_heloc = pd.concat([df_heloc, tdf], axis=1)\n",
        "df_heloc_target_name = 'Risk_Flag'\n",
        "df_heloc.shape"
      ],
      "metadata": {
        "id": "c76DDVX9pBT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fraud Ecomm"
      ],
      "metadata": {
        "id": "xS12ZA9zpGVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "df_fe = pd.read_csv('./data/fraud_ecomm.csv')\n",
        "ip = pd.read_csv('./data/IpAddress_to_Country.csv')\n",
        "df_fe['signup_time'] = pd.to_datetime(df_fe['signup_time'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_fe['signup_time_y'] = df_fe['signup_time'].dt.year\n",
        "df_fe['signup_time_mon'] = df_fe['signup_time'].dt.month\n",
        "df_fe['signup_time_w'] = df_fe['signup_time'].dt.week\n",
        "df_fe['signup_time_d'] = df_fe['signup_time'].dt.day\n",
        "df_fe['signup_time_h'] = df_fe['signup_time'].dt.hour\n",
        "df_fe['signup_time_m'] = df_fe['signup_time'].dt.minute\n",
        "df_fe['signup_time_s'] = df_fe['signup_time'].dt.second\n",
        "df_fe['signup_time_wd'] = df_fe['signup_time'].dt.dayofweek\n",
        "df_fe = df_fe.drop(['signup_time'], axis=1)\n",
        "df_fe['purchase_time'] = pd.to_datetime(df_fe['purchase_time'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_fe['purchase_time_y'] = df_fe['purchase_time'].dt.year\n",
        "df_fe['purchase_time_mon'] = df_fe['purchase_time'].dt.month\n",
        "df_fe['purchase_time_w'] = df_fe['purchase_time'].dt.week\n",
        "df_fe['purchase_time_d'] = df_fe['purchase_time'].dt.day\n",
        "df_fe['purchase_time_h'] = df_fe['purchase_time'].dt.hour\n",
        "df_fe['purchase_time_m'] = df_fe['purchase_time'].dt.minute\n",
        "df_fe['purchase_time_s'] = df_fe['purchase_time'].dt.second\n",
        "df_fe['purchase_time_wd'] = df_fe['purchase_time'].dt.dayofweek\n",
        "df_fe = df_fe.drop(['purchase_time'], axis=1)\n",
        "le = LabelEncoder()\n",
        "df_fe['device_id'] = le.fit_transform(df_fe['device_id'])\n",
        "df_fe['source'] = le.fit_transform(df_fe['source'])\n",
        "df_fe['browser'] = le.fit_transform(df_fe['browser'])\n",
        "df_fe['sex'] = le.fit_transform(df_fe['sex'])\n",
        "df_fe['age'] = le.fit_transform(df_fe['age'])\n",
        "ip['country'] = le.fit_transform(ip['country'])\n",
        "ip['lower_bound_ip_address'] = ip['lower_bound_ip_address'].astype('float')\n",
        "ip['upper_bound_ip_address'] = ip['upper_bound_ip_address'].astype('float')\n",
        "df_fe['ip_address'] = df_fe['ip_address'].astype('float')\n",
        "def ip_to_country(ip_val):\n",
        "    try :\n",
        "        return ip.country[(ip.lower_bound_ip_address < ip_val)                            \n",
        "                                & \n",
        "                                (ip.upper_bound_ip_address > ip_val)].iloc[0]\n",
        "    except IndexError :\n",
        "        return -1\n",
        "df_fe['ip_country'] = df_fe['ip_address'].apply(ip_to_country)\n",
        "\n",
        "device_duplicates = pd.DataFrame(df_fe.groupby(by = \"device_id\").device_id.count())\n",
        "device_duplicates.rename(columns={\"device_id\": \"freq_device\"}, inplace=True)           \n",
        "device_duplicates.reset_index(level=0, inplace= True)\n",
        "df_fe = df_fe.merge(device_duplicates, on= \"device_id\")\n",
        "\n",
        "df_fe = df_fe.drop(['user_id'], axis=1)\n",
        "nf_fe = ['purchase_value', 'ip_address', 'device_id', 'signup_time_y', 'signup_time_mon',\n",
        "       'signup_time_w', 'signup_time_d', 'signup_time_h', 'signup_time_m',\n",
        "       'signup_time_s', 'signup_time_wd', 'purchase_time_y',\n",
        "       'purchase_time_mon', 'purchase_time_w', 'purchase_time_d',\n",
        "       'purchase_time_h', 'purchase_time_m', 'purchase_time_s',\n",
        "       'purchase_time_wd', 'age', 'ip_country', 'freq_device']\n",
        "cf_fe = ['source', 'browser', 'sex']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_fe[nf_fe] = scaler.fit_transform(df_fe[nf_fe])\n",
        "\n",
        "cc_fe = []\n",
        "for cf_name in cf_fe:\n",
        "    df_fe[cf_name] = le.fit_transform(df_fe[cf_name])\n",
        "    cc_fe.append(len(np.unique(df_fe[cf_name])))\n",
        "\n",
        "tdf = df_fe[cf_fe]\n",
        "tdf.index = df_fe.index\n",
        "df_fe = df_fe.drop(cf_fe, axis=1)\n",
        "df_fe = pd.concat([df_fe, tdf], axis=1)\n",
        "\n",
        "df_fe_target_name = 'class'\n",
        "\n",
        "df_fe.shape"
      ],
      "metadata": {
        "id": "vsHeezUapJ6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HPO"
      ],
      "metadata": {
        "id": "HiD04FWA9AQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
        "\n",
        "def objective_c(trial):\n",
        "\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_categorical(\"n\", [100, 200, 500, 1000]),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"lr\", 1e-3, 1, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1e-2, 1),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 1e-2, 1),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 1e-2, 1),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 1),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 1)\n",
        "    }\n",
        "\n",
        "    model = XGBClassifier(**params)\n",
        "    model.fit(X_train_t, y_train_t)\n",
        "    pred = model.predict(X_val_t)\n",
        "    AUC = roc_auc_score(y_val_t, pred)\n",
        "    return AUC\n",
        "     "
      ],
      "metadata": {
        "id": "cmVC-88JSMwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_vld.drop([df_vld_target_name], axis=1)\n",
        "y = df_vld[df_vld_target_name]\n",
        "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective_c, n_trials=100)\n",
        "print(study.best_trial.params)\n",
        "print(study.best_value)"
      ],
      "metadata": {
        "id": "jclRkBpwRzcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_r(trial):\n",
        "\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_categorical(\"n\", [5, 10, 50, 100, 200, 500]),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "        \"learning_rate\": trial.suggest_float(\"lr\", 1e-3, 1, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1e-2, 1),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 1e-2, 1),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 1e-2, 1),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 1),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 1)\n",
        "    }\n",
        "\n",
        "    model = XGBRegressor(**params)\n",
        "    model.fit(X_train_t, y_train_t)\n",
        "    pred = model.predict(X_val_t)\n",
        "    RMSE = mean_squared_error(y_val_t, pred, squared=False)\n",
        "    return RMSE\n",
        "     "
      ],
      "metadata": {
        "id": "MxntMsSL6V2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_bh.drop([df_bh_target_name], axis=1)\n",
        "y = df_bh[df_bh_target_name]\n",
        "X_train_t, X_val_t, y_train_t, y_val_t = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
        "study.optimize(objective_r, n_trials=100)\n",
        "print(study.best_trial.params)\n",
        "print(study.best_value)"
      ],
      "metadata": {
        "id": "Ffn00DdWATem"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}