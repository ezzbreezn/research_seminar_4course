{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8YmfTf_YDhM"
      },
      "outputs": [],
      "source": [
        "!pip install optuna rtdl category_encoders ruamel.yaml einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import fmin\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import pathlib\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint as cp\n",
        "from tqdm import tqdm, trange\n",
        "from typing import Optional, Sequence, Tuple, Union, Any, Dict, List\n",
        "from copy import deepcopy\n",
        "import enum\n",
        "import optuna\n",
        "import rtdl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "import category_encoders as ce\n",
        "import ruamel.yaml\n",
        "import math\n",
        "from collections import OrderedDict, defaultdict\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, accuracy_score, recall_score, roc_auc_score, balanced_accuracy_score, log_loss, mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from scipy.spatial import distance_matrix\n",
        "from scipy.linalg import qr\n",
        "from torch.autograd import Function\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "8KzmCO1TYL_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_global_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "set_global_seed(42)"
      ],
      "metadata": {
        "id": "28nvIY89YN1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "UJFgccj6YOmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessings"
      ],
      "metadata": {
        "id": "8s_vn0xeYQTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sin(x: torch.Tensor) -> torch.Tensor:\n",
        "    return torch.cat([torch.cos(x), torch.sin(x)], -1)\n",
        "\n",
        "def positional(d, pos):\n",
        "    return torch.Tensor([np.sin(1/10000 ** (2 * int(i / 2) / d) * pos) if i % 2 == 0 else \n",
        "                       np.cos(1/10000 ** (2 * int(i / 2) / d) * pos) for i in range(d)])\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, n_features: int = 5000, dropout: float = 0.1, tf: bool=False):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.tf = tf\n",
        "        self.d_model = d_model\n",
        "        position = torch.arange(n_features).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(n_features, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "        self.pe = pe\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.tf:\n",
        "            x = x + self.pe.unsqueeze(0)\n",
        "        else:\n",
        "            x = x + self.pe.view(1, -1)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Periodic(nn.Module):\n",
        "    def __init__(self, n_features: int, n: int, sigma: float, trainable: bool, initialization: str, tf: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.tf = tf\n",
        "        if initialization == 'log-linear':\n",
        "            coefficients = sigma ** (torch.arange(n) / n)\n",
        "            coefficients = coefficients[None].repeat(n_features, 1)\n",
        "        else:\n",
        "            assert initialization == 'normal'\n",
        "            coefficients = torch.normal(0.0, sigma, (n_features, n))\n",
        "        if trainable:\n",
        "            self.coefficients = nn.Parameter(coefficients) \n",
        "        else:\n",
        "            self.register_buffer('coefficients', coefficients)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if not self.tf:\n",
        "            return cos_sin(2 * np.pi * self.coefficients[None] * x[..., None]).view(-1, 2 * x.shape[1] * self.coefficients.shape[1])\n",
        "        else:\n",
        "            return cos_sin(2 * np.pi * self.coefficients[None] * x[..., None]).view(x.shape[0], x.shape[1], 2 * self.coefficients.shape[1])\n",
        "    \n",
        "    \n",
        "class NLinear(nn.Module):\n",
        "    def __init__(self, n: int, d_in: int, d_out: int, bias: bool = True) -> None:\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.Tensor(n, d_in, d_out))\n",
        "        self.bias = nn.Parameter(torch.Tensor(n, d_out)) if bias else None\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                layer = nn.Linear(d_in, d_out)\n",
        "                self.weight[i] = layer.weight.T\n",
        "                if self.bias is not None:\n",
        "                    self.bias[i] = layer.bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 3:\n",
        "            x = x[..., None] * self.weight[None]\n",
        "            x = x.sum(-2)\n",
        "            if self.bias is not None:\n",
        "                x = x + self.bias[None]\n",
        "            return x\n",
        "    \n",
        "\n",
        "class FeaturesTokenizer(nn.Module):\n",
        "    def __init__(self, n_features: int, d_embedding: int, tf: bool) -> None:\n",
        "        super().__init__()\n",
        "        self.tf = tf\n",
        "        self.first_layer = rtdl.NumericalFeatureTokenizer(n_features, d_embedding, True, 'uniform')\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.first_layer(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "class LinearEmbeddings(nn.Module):\n",
        "    def __init__(self, n_layers, n_features, d_embeddings, tf):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.tf = tf\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(n_layers):\n",
        "            if i == 0:\n",
        "                self.layers.append(rtdl.NumericalFeatureTokenizer(n_features, d_embeddings[i], False, 'uniform'))\n",
        "            else:\n",
        "                self.layers.append(NLinear(n_features, d_embeddings[i-1], d_embeddings[i], False))\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.layers[i](x)\n",
        "            x = self.leaky_relu(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x    \n",
        "    \n",
        "    \n",
        "class AutoDis(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_features: int, d_embedding: int, n_meta_embeddings: int, temperature: float, tf: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.first_layer = rtdl.NumericalFeatureTokenizer(\n",
        "            n_features,\n",
        "            n_meta_embeddings,\n",
        "            False,\n",
        "            'uniform',\n",
        "        )\n",
        "        self.tf = tf\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "        self.second_layer = NLinear(\n",
        "            n_features, n_meta_embeddings, n_meta_embeddings, False\n",
        "        )\n",
        "        self.softmax = nn.Softmax(-1)\n",
        "        self.temperature = temperature\n",
        "        self.third_layer = NLinear(\n",
        "            n_features, n_meta_embeddings, d_embedding, False\n",
        "        )\n",
        "        nn.init.uniform_(self.third_layer.weight, 0.01)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.first_layer(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.second_layer(x)\n",
        "        x = self.softmax(x / self.temperature)\n",
        "        x = self.third_layer(x)\n",
        "        if not self.tf:\n",
        "            return x.view(x.shape[0], -1)\n",
        "        else:\n",
        "            return x\n",
        "    \n",
        "    \n",
        "class SoftEmbedding(torch.nn.Module):\n",
        "    def __init__(self, num_embeddings, embeddings_dim, emb_initializer=None, tf=True):\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.embedding_table = torch.nn.Embedding(num_embeddings, embeddings_dim)\n",
        "        if emb_initializer:\n",
        "            emb_initializer(self.embedding_table.weight)\n",
        "        self.projection_layer = torch.nn.Linear(1, num_embeddings, bias=True)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.tf = tf\n",
        "\n",
        "    def forward(self, input_numeric):\n",
        "        input_numeric = input_numeric.unsqueeze(-1)\n",
        "        weights = self.softmax(self.projection_layer(input_numeric))\n",
        "        soft_one_hot_embeddings = (weights.unsqueeze(-1) * self.embedding_table.weight).sum(-2)\n",
        "        if not self.tf:\n",
        "            return soft_one_hot_embeddings.view(soft_one_hot_embeddings.shape[0], -1)\n",
        "        else:\n",
        "            return soft_one_hot_embeddings\n",
        "\n",
        "\n",
        "class EntmaxBisectFunction(Function):\n",
        "    @classmethod\n",
        "    def _gp(cls, x, alpha):\n",
        "        return x ** (alpha - 1)\n",
        "\n",
        "    @classmethod\n",
        "    def _gp_inv(cls, y, alpha):\n",
        "        return y ** (1 / (alpha - 1))\n",
        "\n",
        "    @classmethod\n",
        "    def _p(cls, X, alpha):\n",
        "        return cls._gp_inv(torch.clamp(X, min=0), alpha)\n",
        "\n",
        "    @classmethod\n",
        "    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\n",
        "\n",
        "        if not isinstance(alpha, torch.Tensor):\n",
        "            alpha = torch.tensor(alpha, dtype=X.dtype, device=X.device)\n",
        "\n",
        "        alpha_shape = list(X.shape)\n",
        "        alpha_shape[dim] = 1\n",
        "        alpha = alpha.expand(*alpha_shape)\n",
        "\n",
        "        ctx.alpha = alpha\n",
        "        ctx.dim = dim\n",
        "        d = X.shape[dim]\n",
        "\n",
        "        X = X * (alpha - 1)\n",
        "\n",
        "        max_val, _ = X.max(dim=dim, keepdim=True)\n",
        "\n",
        "        tau_lo = max_val - cls._gp(1, alpha)\n",
        "        tau_hi = max_val - cls._gp(1 / d, alpha)\n",
        "\n",
        "        f_lo = cls._p(X - tau_lo, alpha).sum(dim) - 1\n",
        "\n",
        "        dm = tau_hi - tau_lo\n",
        "\n",
        "        for it in range(n_iter):\n",
        "\n",
        "            dm /= 2\n",
        "            tau_m = tau_lo + dm\n",
        "            p_m = cls._p(X - tau_m, alpha)\n",
        "            f_m = p_m.sum(dim) - 1\n",
        "\n",
        "            mask = (f_m * f_lo >= 0).unsqueeze(dim)\n",
        "            tau_lo = torch.where(mask, tau_m, tau_lo)\n",
        "\n",
        "        if ensure_sum_one:\n",
        "            p_m /= p_m.sum(dim=dim).unsqueeze(dim=dim)\n",
        "\n",
        "        ctx.save_for_backward(p_m)\n",
        "\n",
        "        return p_m\n",
        "\n",
        "    @classmethod\n",
        "    def backward(cls, ctx, dY):\n",
        "        Y, = ctx.saved_tensors\n",
        "\n",
        "        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))\n",
        "\n",
        "        dX = dY * gppr\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
        "        q = q.unsqueeze(ctx.dim)\n",
        "        dX -= q * gppr\n",
        "\n",
        "        d_alpha = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "\n",
        "            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))\n",
        "            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "\n",
        "            d_alpha = dY * (Y - Y_skewed) / ((ctx.alpha - 1) ** 2)\n",
        "            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)\n",
        "            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)\n",
        "\n",
        "        return dX, d_alpha, None, None, None\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):\n",
        "    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)\n",
        "\n",
        "        \n",
        "        \n",
        "class EntmaxBisect(nn.Module):\n",
        "    def __init__(self, alpha=1.5, dim=-1, n_iter=50):\n",
        "        self.dim = dim\n",
        "        self.n_iter = n_iter\n",
        "        self.alpha = alpha\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, X):\n",
        "        return entmax_bisect(\n",
        "            X, alpha=self.alpha, dim=self.dim, n_iter=self.n_iter\n",
        "        )\n",
        "  \n",
        "        \n",
        "class SparseAttLayer(nn.Module):\n",
        "    def __init__(self, nhead: int, nfield: int, nemb: int, d_k: int, nhid: int, alpha: float = 1.5):\n",
        "        \"\"\" Multi-Head Sparse Attention Layer \"\"\"\n",
        "        super(SparseAttLayer, self).__init__()\n",
        "        self.sparsemax = nn.Softmax(dim=-1) if alpha == 1. \\\n",
        "            else EntmaxBisect(alpha, dim=-1)\n",
        "\n",
        "        self.scale = d_k ** -0.5\n",
        "        self.bilinear_w = nn.Parameter(torch.zeros(nhead, nemb, d_k))                   \n",
        "        self.query = nn.Parameter(torch.zeros(nhead, nhid, d_k))                        \n",
        "        self.values = nn.Parameter(torch.zeros(nhead, nhid, nfield))                    \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.xavier_uniform_(self.bilinear_w, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.query, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.values, gain=1.414)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x                                                                        \n",
        "        att_gates = torch.einsum('bfx,kxy,koy->bkof',\n",
        "                                 keys, self.bilinear_w, self.query) * self.scale        \n",
        "        sparse_gates = self.sparsemax(att_gates)                                        \n",
        "        return torch.einsum('bkof,kof->bkof', sparse_gates, self.values)\n",
        "\n",
        "    \n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, nfeat, nemb):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(nfeat, nemb)\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x['id'])                           \n",
        "        return emb * x['value'].unsqueeze(2)                    \n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dim_head = 16,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
        "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        dropped_attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h i j, b h j d -> b h i d', dropped_attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
        "        return self.to_out(out), attn\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_tokens, dim, depth, heads, dim_head, attn_dropout, ff_dropout):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.Embedding(num_tokens, dim)\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, return_attn = False):\n",
        "        x = self.embeds(x)\n",
        "\n",
        "        post_softmax_attns = []\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            attn_out, post_softmax_attn = attn(x)\n",
        "            post_softmax_attns.append(post_softmax_attn)\n",
        "\n",
        "            x = x + attn_out\n",
        "            x = ff(x) + x\n",
        "\n",
        "        if not return_attn:\n",
        "            return x\n",
        "\n",
        "        return x, torch.stack(post_softmax_attns)\n",
        "\n",
        "class MLP_TT(nn.Module):\n",
        "    def __init__(self, dims, act = None):\n",
        "        super().__init__()\n",
        "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
        "        layers = []\n",
        "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
        "            is_last = ind >= (len(dims_pairs) - 1)\n",
        "            linear = nn.Linear(dim_in, dim_out)\n",
        "            layers.append(linear)\n",
        "\n",
        "            if is_last:\n",
        "                continue\n",
        "\n",
        "            act = default(act, nn.ReLU())\n",
        "            layers.append(act)\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class TabTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        categories,\n",
        "        num_continuous,\n",
        "        dim, \n",
        "        depth, \n",
        "        heads, \n",
        "        dim_head = 16,\n",
        "        dim_out = 1,\n",
        "        mlp_hidden_mults = (4, 2),\n",
        "        mlp_act = None,\n",
        "        num_special_tokens = 2,\n",
        "        continuous_mean_std = None,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_categories = len(categories)\n",
        "        self.num_unique_categories = sum(categories)\n",
        "\n",
        "        self.num_special_tokens = num_special_tokens\n",
        "        total_tokens = self.num_unique_categories + num_special_tokens\n",
        "\n",
        "        if self.num_unique_categories > 0:\n",
        "            categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
        "            categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
        "            self.register_buffer('categories_offset', categories_offset)\n",
        "\n",
        "        self.num_continuous = num_continuous\n",
        "\n",
        "        if self.num_continuous > 0:\n",
        "            if exists(continuous_mean_std):\n",
        "                assert continuous_mean_std.shape == (num_continuous, 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
        "            self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
        "\n",
        "            self.norm = nn.LayerNorm(num_continuous)\n",
        "\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            num_tokens = total_tokens,\n",
        "            dim = dim,\n",
        "            depth = depth,\n",
        "            heads = heads,\n",
        "            dim_head = dim_head,\n",
        "            attn_dropout = attn_dropout,\n",
        "            ff_dropout = ff_dropout\n",
        "        )\n",
        "\n",
        "        input_size = (dim * self.num_categories) + num_continuous\n",
        "        l = input_size // 8\n",
        "\n",
        "        hidden_dimensions = list(map(lambda t: l * t, mlp_hidden_mults))\n",
        "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
        "        \n",
        "        \n",
        "        self.mlp = MLP_TT(all_dimensions, act = mlp_act)\n",
        "\n",
        "    def forward(self, x_categ, x_cont, return_attn = False):\n",
        "        xs = []\n",
        "\n",
        "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
        "\n",
        "        if self.num_unique_categories > 0:\n",
        "            x_categ += self.categories_offset\n",
        "\n",
        "            x, attns = self.transformer(x_categ, return_attn = True)\n",
        "            xs.append(x)\n",
        "        if self.num_continuous > 0:\n",
        "            if exists(self.continuous_mean_std):\n",
        "                mean, std = self.continuous_mean_std.unbind(dim = -1)\n",
        "                x_cont = (x_cont - mean) / std\n",
        "\n",
        "            normed_cont = self.norm(x_cont)\n",
        "            xs.append(normed_cont)\n",
        "\n",
        "        x = torch.cat(xs, dim = -1)\n",
        "        return x\n",
        "        \n"
      ],
      "metadata": {
        "id": "9BddsemuYT5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "bptpdcZQYWlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _TokenInitialization(enum.Enum):\n",
        "    UNIFORM = 'uniform'\n",
        "    NORMAL = 'normal'\n",
        "\n",
        "    @classmethod\n",
        "    def from_str(cls, initialization: str) -> '_TokenInitialization':\n",
        "        try:\n",
        "            return cls(initialization)\n",
        "        except ValueError:\n",
        "            valid_values = [x.value for x in _TokenInitialization]\n",
        "            raise ValueError(f'initialization must be one of {valid_values}')\n",
        "\n",
        "    def apply(self, x: torch.Tensor, d: int) -> None:\n",
        "        d_sqrt_inv = 1 / math.sqrt(d)\n",
        "        if self == _TokenInitialization.UNIFORM:\n",
        "            nn.init.uniform_(x, a=-d_sqrt_inv, b=d_sqrt_inv)\n",
        "        elif self == _TokenInitialization.NORMAL:\n",
        "            nn.init.normal_(x, std=d_sqrt_inv)\n",
        "\n",
        "class CategoricalFeatureTokenizer(nn.Module):\n",
        "    category_offsets: torch.Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        cardinalities: List[int],\n",
        "        d_token: int,\n",
        "        bias: bool,\n",
        "        initialization: str,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert cardinalities, 'cardinalities must be non-empty'\n",
        "        assert d_token > 0, 'd_token must be positive'\n",
        "        initialization_ = _TokenInitialization.from_str(initialization)\n",
        "\n",
        "        category_offsets = torch.tensor([0] + cardinalities[:-1]).cumsum(0)\n",
        "        self.register_buffer('category_offsets', category_offsets, persistent=False)\n",
        "        self.embeddings = nn.Embedding(sum(cardinalities), d_token)\n",
        "        self.bias = nn.Parameter(torch.Tensor(len(cardinalities), d_token)) if bias else None\n",
        "\n",
        "        for parameter in [self.embeddings.weight, self.bias]:\n",
        "            if parameter is not None:\n",
        "                initialization_.apply(parameter, d_token)\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        return len(self.category_offsets)\n",
        "\n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        return self.embeddings.embedding_dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.embeddings(x + self.category_offsets[None])\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias[None]\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureTokenizer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_num_features: int,\n",
        "        cat_cardinalities: List[int],\n",
        "        d_token: int,\n",
        "        preproc_type: str,\n",
        "        cat_preproc_type: str,\n",
        "        preproc_args: dict,\n",
        "        positional: bool,\n",
        "        tf: bool\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert n_num_features >= 0, 'n_num_features must be non-negative'\n",
        "        assert (\n",
        "            n_num_features or cat_cardinalities\n",
        "        ), 'at least one of n_num_features or cat_cardinalities must be positive/non-empty'\n",
        "        self.initialization = 'uniform'\n",
        "        self.n_num_features = n_num_features\n",
        "        self.d = d_token\n",
        "        self.tf = tf\n",
        "        self.args = preproc_args\n",
        "        self.preproc_type = preproc_type\n",
        "        self.cat_preproc_type = cat_preproc_type\n",
        "        self.arm_num = None\n",
        "        self.arm_num_cat = None\n",
        "        if preproc_type == 'ARM' or preproc_type == 'ARM_Bin':\n",
        "            self.attn_layer = SparseAttLayer(8, n_num_features, d_token, d_token, 16, 1.7)\n",
        "            self.arm_num = 8 * self.d * 16 \n",
        "        if n_num_features:\n",
        "            if preproc_type == 'Periodic':\n",
        "                self.num_tokenizer = Periodic(**preproc_args)\n",
        "                self.d = self.d * 2\n",
        "            elif preproc_type == 'Linear':\n",
        "                self.num_tokenizer = LinearEmbeddings(**preproc_args)\n",
        "            elif preproc_type == 'AutoDis':\n",
        "                self.num_tokenizer = AutoDis(**preproc_args)\n",
        "            elif preproc_type == 'Tokens' or preproc_type == 'ARM':\n",
        "                self.num_tokenizer = FeaturesTokenizer(**preproc_args)\n",
        "            elif preproc_type == 'SoftEmbedding':\n",
        "                self.num_tokenizer = SoftEmbedding(**preproc_args)\n",
        "            elif preproc_type == 'BinEncoding' or preproc_type == 'ARM_Bin':\n",
        "                self.num_tokenizer = BinEncoding(**preproc_args)\n",
        "            elif preproc_type == 'None':\n",
        "                self.num_tokenizer = None\n",
        "        else:\n",
        "            self.num_tokenizer = None\n",
        "        if cat_cardinalities is None:\n",
        "            self.cat_tokenizer = None\n",
        "        elif cat_preproc_type == 'Lookup' or cat_preproc_type == 'ARM': \n",
        "            self.cat_tokenizer = (\n",
        "                CategoricalFeatureTokenizer(\n",
        "                    cat_cardinalities, self.d, True, self.initialization\n",
        "                )\n",
        "                if cat_cardinalities\n",
        "                else None\n",
        "            )\n",
        "        elif cat_preproc_type == 'TT':\n",
        "            self.cat_tokenizer = (\n",
        "                TabTransformer(\n",
        "                    num_continuous=0,\n",
        "                    categories=cat_cardinalities, \n",
        "                    dim=self.d, \n",
        "                    depth=3,\n",
        "                    heads=8,\n",
        "                )\n",
        "                if cat_cardinalities\n",
        "                else None\n",
        "            )\n",
        "        if cat_preproc_type == 'ARM':\n",
        "            self.arm_num_cat = (self.n_num_features + 8 * 16) * self.d\n",
        "            self.cat_attn_layer = SparseAttLayer(8, len(cat_cardinalities), self.d, self.d, 16, 1.7)\n",
        "        if positional:\n",
        "            self.positional = PositionalEncoding(self.d, self.n_num_features, 0.1, True)\n",
        "        else:\n",
        "            self.positional = None\n",
        "\n",
        "    @property\n",
        "    def n_tokens(self) -> int:\n",
        "        return self.n_num_features + self.cat_tokenizer.n_tokens\n",
        "        \n",
        "    @property\n",
        "    def d_token(self) -> int:\n",
        "        return self.d\n",
        "\n",
        "    def forward(self, x_num: Optional[torch.Tensor], x_cat: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "        if self.num_tokenizer is None:\n",
        "            return x_num[..., None]\n",
        "        assert (\n",
        "            x_num is not None or x_cat is not None\n",
        "        ), 'At least one of x_num and x_cat must be presented'\n",
        "        assert _all_or_none(\n",
        "            [self.num_tokenizer, x_num]\n",
        "        ), 'If self.num_tokenizer is (not) None, then x_num must (not) be None'\n",
        "        assert _all_or_none(\n",
        "            [self.cat_tokenizer, x_cat]\n",
        "        ), 'If self.cat_tokenizer is (not) None, then x_cat must (not) be None'\n",
        "        x = []\n",
        "        if self.num_tokenizer is not None:\n",
        "            if self.positional is not None:\n",
        "                x.append(self.positional(self.num_tokenizer(x_num)))\n",
        "            else:\n",
        "                x.append(self.num_tokenizer(x_num))\n",
        "        if self.cat_tokenizer is not None:\n",
        "            if self.cat_preproc_type == 'TT':\n",
        "                x.append(self.cat_tokenizer(x_cat, None))\n",
        "            elif self.cat_preproc_type == 'ARM':\n",
        "                temp = self.cat_tokenizer(x_cat)\n",
        "                w = self.cat_attn_layer(temp)\n",
        "                w = torch.clamp(w, -1e5, 1e5)\n",
        "                x_cat_arm = torch.exp(torch.einsum('bfe,bkof->bkoe', temp, w))          \n",
        "                x_cat_arm = rearrange(x_cat_arm, 'b k o e -> b (k o) e')\n",
        "                x_cat_arm = torch.clamp(x_cat_arm, -1e5, 1e5)\n",
        "                x.append(x_cat_arm)\n",
        "            else:\n",
        "                x.append(self.cat_tokenizer(x_cat))\n",
        "\n",
        "        if self.preproc_type == 'ARM' or self.preproc_type == 'ARM_Bin':\n",
        "            e = x[0]\n",
        "            arm_weight = self.attn_layer(e)\n",
        "            arm_weight = torch.clamp(arm_weight, -1e5, 1e5)                             \n",
        "            x_arm = torch.exp(torch.einsum('bfe,bkof->bkoe', e, arm_weight))         \n",
        "            x_arm = rearrange(x_arm, 'b k o e -> b (k o) e')\n",
        "            x_arm = torch.clamp(x_arm, -1e5, 1e5)\n",
        "            x[0] = x_arm\n",
        "            x = torch.cat(x, dim=1)\n",
        "            if not self.tf:\n",
        "                return x.reshape(x.shape[0], -1)\n",
        "            else:\n",
        "                return x\n",
        "        if not self.tf:\n",
        "            return x[0].view(x[0].shape[0], -1) if len(x) == 1 else torch.cat(x, dim=1).view(x[0].shape[0], -1)\n",
        "        else:\n",
        "            return x[0] if len(x) == 1 else torch.cat(x, dim=1)\n"
      ],
      "metadata": {
        "id": "d76C-LBEYVnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union, cast\n",
        "\n",
        "def reglu(x: torch.Tensor) -> torch.Tensor:\n",
        "    assert x.shape[-1] % 2 == 0\n",
        "    a, b = x.chunk(2, dim=-1)\n",
        "    return a * F.relu(b)\n",
        "\n",
        "\n",
        "def geglu(x: torch.Tensor) -> torch.Tensor:\n",
        "    assert x.shape[-1] % 2 == 0\n",
        "    a, b = x.chunk(2, dim=-1)\n",
        "    return a * F.gelu(b)\n",
        "\n",
        "\n",
        "\n",
        "ModuleType = Union[str, Callable[..., nn.Module]]\n",
        "_INTERNAL_ERROR_MESSAGE = 'Internal error. Please, open an issue.'\n",
        "\n",
        "\n",
        "def _is_glu_activation(activation: ModuleType):\n",
        "    return (\n",
        "        isinstance(activation, str)\n",
        "        and activation.endswith('GLU')\n",
        "        or activation in [ReGLU, GEGLU]\n",
        "    )\n",
        "\n",
        "\n",
        "def _all_or_none(values):\n",
        "    return all(x is None for x in values) or all(x is not None for x in values)\n",
        "\n",
        "\n",
        "class ReGLU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return reglu(x)\n",
        "\n",
        "\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return geglu(x)\n"
      ],
      "metadata": {
        "id": "ZUDoodbXYal6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNoise(nn.Module):\n",
        "    def __init__(self, stddev, device):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, din):\n",
        "        if self.training:\n",
        "            return din + torch.autograd.Variable(\n",
        "                torch.randn(din.size()).to(self.device) * self.stddev\n",
        "            )\n",
        "        return din\n",
        "\n",
        "class DenseLightBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_in,\n",
        "        n_out,\n",
        "        drop_rate=0.1,\n",
        "        noise_std=0.05,\n",
        "        act_fun=nn.ReLU,\n",
        "        use_bn=True,\n",
        "        use_noise=True,\n",
        "        use_dropout=True,\n",
        "        use_act=True,\n",
        "        device=torch.device(\"cuda:0\"),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(DenseLightBlock, self).__init__()\n",
        "        self.features = nn.Sequential(OrderedDict([]))\n",
        "\n",
        "        if use_bn:\n",
        "            self.features.add_module(\"norm\", nn.BatchNorm1d(n_in))\n",
        "        if use_dropout and drop_rate:\n",
        "            self.features.add_module(\"dropout\", nn.Dropout(p=drop_rate))\n",
        "        if use_noise:\n",
        "            self.features.add_module(\"noise\", GaussianNoise(noise_std, device))\n",
        "\n",
        "        self.features.add_module(\"dense\", nn.Linear(n_in, n_out))\n",
        "\n",
        "        if use_act:\n",
        "            self.features.add_module(\"act\", act_fun())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for name, layer in self.features.named_children():\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DenseLightModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_in,\n",
        "        n_out=1,\n",
        "        hidden_size=[\n",
        "            512,\n",
        "            512,\n",
        "            512,\n",
        "        ],\n",
        "        drop_rate=[\n",
        "            0.1,\n",
        "            0.1,\n",
        "            0.1,\n",
        "        ],\n",
        "        act_fun=nn.ReLU,\n",
        "        noise_std=0.05,\n",
        "        bias=None,\n",
        "        num_init_features=None,\n",
        "        use_bn=True,\n",
        "        use_noise=True,\n",
        "        use_dropout=True,\n",
        "        use_act=True,\n",
        "        concat_input=True,\n",
        "        device=torch.device(\"cuda:0\"),\n",
        "        preproc=False,\n",
        "        preproc_type=None,\n",
        "        preproc_args=None,\n",
        "        positional=False,\n",
        "        tokenizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(DenseLightModel, self).__init__()\n",
        "        \n",
        "        if isinstance(drop_rate, float):\n",
        "            drop_rate = [drop_rate] * len(hidden_size)\n",
        "            \n",
        "        assert len(hidden_size) == len(\n",
        "            drop_rate\n",
        "        ), \"Wrong number hidden_sizes/drop_rates. Must be equal.\"\n",
        "\n",
        "        self.concat_input = concat_input\n",
        "        num_features = n_in if num_init_features is None else num_init_features\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([]))\n",
        "        self.d_embedding = None\n",
        "        self.tokenizer = tokenizer\n",
        "        if tokenizer is not None:\n",
        "            self.features.add_module('preproc', tokenizer)\n",
        "            if tokenizer.arm_num is not None:\n",
        "                num_features = tokenizer.arm_num\n",
        "            elif tokenizer.arm_num_cat is not None:\n",
        "                num_features = tokenizer.arm_num_cat\n",
        "            else:\n",
        "                num_features = num_features * tokenizer.d\n",
        "            self.d_embedding = tokenizer.d\n",
        "        if preproc:\n",
        "            if preproc_type == 'Periodic':\n",
        "                self.features.add_module('preproc', Periodic(**preproc_args))\n",
        "                num_features = 2 * num_features * preproc_args['n']\n",
        "                self.d_embedding = preproc_args['n']\n",
        "            elif preproc_type == 'Linear':\n",
        "                self.features.add_module('preproc', LinearEmbeddings(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embeddings'][-1]\n",
        "                self.d_embedding = preproc_args['d_embeddings'][-1]\n",
        "            elif preproc_type == 'AutoDis':\n",
        "                self.features.add_module('preproc', AutoDis(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embedding']\n",
        "                self.d_embedding = preproc_args['d_embedding']\n",
        "            elif preproc_type == 'Tokens':\n",
        "                self.features.add_module('preproc', FeaturesTokenizer(**preproc_args))\n",
        "                num_features = num_features * preproc_args['d_embedding']\n",
        "                self.d_embedding = preproc_args['d_embedding']\n",
        "            elif preproc_type == 'SoftEmbedding':\n",
        "                self.features.add_module('preproc', SoftEmbedding(**preproc_args))\n",
        "                num_features = num_features * preproc_args['embeddings_dim']\n",
        "                self.d_embedding = preproc_args['embeddings_dim']\n",
        "        if positional:\n",
        "            self.features.add_module('positional', PositionalEncoding(self.d_embedding, n_in, 0.1, False))\n",
        "        if num_init_features is not None:\n",
        "            self.features.add_module(\"dense0\", nn.Linear(n_in, num_features))\n",
        "\n",
        "        for i, hid_size in enumerate(hidden_size):\n",
        "            block = DenseLightBlock(\n",
        "                n_in=num_features,\n",
        "                n_out=hid_size,\n",
        "                drop_rate=drop_rate[i] if use_dropout else 0,\n",
        "                noise_std=noise_std,\n",
        "                act_fun=act_fun,\n",
        "                use_bn=use_bn,\n",
        "                use_noise=use_noise,\n",
        "                use_dropout=use_dropout,\n",
        "                use_act=use_act,\n",
        "                device=device,\n",
        "            )\n",
        "            self.features.add_module(\"denseblock%d\" % (i + 1), block)\n",
        "\n",
        "            if concat_input:\n",
        "                num_features = n_in + hid_size\n",
        "            else:\n",
        "                num_features = hid_size\n",
        "\n",
        "        num_features = hidden_size[-1]\n",
        "        self.fc = nn.Linear(num_features, n_out)\n",
        "\n",
        "        if bias is not None:\n",
        "            print(\"init bias!\")\n",
        "            bias = torch.Tensor(bias)\n",
        "            self.fc.bias.data = bias\n",
        "            self.fc.weight.data = torch.zeros(n_out, num_features, requires_grad=True)\n",
        "\n",
        "    def forward(self, x, x_cat=None):\n",
        "        for name, layer in self.features.named_children():\n",
        "            if name == 'preproc' and self.tokenizer is not None:\n",
        "                x = layer(x, x_cat)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(DenseLightModel):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MLP, self).__init__(*args, **{**kwargs, **{\"concat_input\": False}})\n",
        "\n",
        "    def forward(self, x, x_cat=None):\n",
        "        return super(MLP, self).forward(x, x_cat)\n"
      ],
      "metadata": {
        "id": "a3vBpXL8YcZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "_IJXW4jJYfTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric():\n",
        "    def __init__(self, metric, higher_is_better=True, name='name', optimize=False, discrete=False, **kwargs):\n",
        "        self.name = name\n",
        "        self.higher_is_better = higher_is_better\n",
        "        self.optimize = optimize\n",
        "        self.discrete = discrete\n",
        "        self.metric = metric\n",
        "        self.best_thr = 0.5\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.name)\n",
        "\n",
        "    def __call__(self, y_true, y_pred, thr=0.5, use_best=False):\n",
        "        if self.discrete:\n",
        "            return self.metric(y_true, y_pred, thr=thr if not use_best else self.best_thr)\n",
        "        else:\n",
        "            return self.metric(y_true, y_pred)\n",
        "\n",
        "    def find_threshold(self, y_true, y_pred):\n",
        "        if self.optimize:\n",
        "            w0 = [0.5]\n",
        "            res = fmin(self.opt, w0, args=(y_true, y_pred), disp=0)[0]\n",
        "            self.best_thr = res\n",
        "            return res\n",
        "        else:\n",
        "            return 0.5\n",
        "\n",
        "    def opt(self, w, y_true, y_pred):\n",
        "        return (-1) ** (self.higher_is_better) * self(y_true, y_pred, w[0])\n",
        "\n",
        "\n",
        "def f1_custom(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='micro')\n",
        "\n",
        "\n",
        "def f1_macro(y_true, y_pred, thr=0.5):\n",
        "    return f1_score(y_true, y_pred > thr, average='macro')\n",
        "\n",
        "\n",
        "def acc_score(y_true, y_pred, thr=0.5):\n",
        "    return accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "def bacc_score(y_true, y_pred, thr=0.5):\n",
        "    return balanced_accuracy_score(y_true, y_pred > thr)\n",
        "\n",
        "\n",
        "class MetricFactory:\n",
        "    def __init__(self, ):\n",
        "        self.metrics = {\n",
        "            'auc': Metric(metric=roc_auc_score, higher_is_better=True, name='auc', optimize=False, discrete=False),\n",
        "            'log-loss': Metric(metric=log_loss, higher_is_better=False, name='log-loss', optimize=False,\n",
        "                              discrete=False),\n",
        "            'f1': Metric(metric=f1_custom, higher_is_better=True, name='f1', optimize=True, discrete=True),\n",
        "            'f1-macro': Metric(metric=f1_macro, higher_is_better=True, name='f1_macro', optimize=True, discrete=True),\n",
        "            'balanced-acc': Metric(metric=bacc_score, higher_is_better=True, name='balanced-acc', optimize=True,\n",
        "                                   discrete=True),\n",
        "            'acc': Metric(metric=acc_score, higher_is_better=True, name='acc', optimize=True, discrete=True),\n",
        "            'mse': Metric(metric=mean_squared_error, higher_is_better=False, name='mse', optimize=False, discrete=False),\n",
        "            'r2': Metric(metric=r2_score, higher_is_better=True, name='r2', optimize=False, discrete=False),\n",
        "            'mae': Metric(metric=mean_absolute_error, higher_is_better=False, name='mae', optimize=False, discrete=False)\n",
        "        }\n",
        "\n",
        "    def get_allowed(self):\n",
        "        return sorted(list(self.metrics.keys()))\n",
        "\n",
        "    def add(self, metric_name, metric_class):\n",
        "        self.metrics[metric_name] = metric_class\n",
        "        return self\n",
        "\n",
        "    def remove(self, metric_name):\n",
        "        del self.models[metric_name]\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self, metric_name):\n",
        "        return deepcopy(self.metrics[metric_name])\n"
      ],
      "metadata": {
        "id": "ixsnWZDhYeKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluation"
      ],
      "metadata": {
        "id": "-UQaTd-yYjMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(\n",
        "    outputs,\n",
        "    targets,\n",
        ") -> Dict[str, float]:\n",
        "    metrics = {}\n",
        "    \n",
        "    y_true = np.array(targets.cpu())\n",
        "    y_pred = (np.array(outputs.detach().cpu()))\n",
        "    \n",
        "    auc = metric_factory['auc']\n",
        "    auc_score = auc(y_true, y_pred)\n",
        "    metrics['auc'] = auc_score\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "CQv-LHkAYg_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ftt(\n",
        "    model: nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    silent: bool\n",
        ") -> None:\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "    if not silent:\n",
        "        for i, (data, targets) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "        ):\n",
        "\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            data_num = data\n",
        "            data_cat = None\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data_num, data_cat)\n",
        "            loss = criterion(pred, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    else:\n",
        "        for i, (data, targets) in enumerate(dataloader):\n",
        "\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            data_num = data\n",
        "            data_cat = None\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(data_num, data_cat)\n",
        "            loss = criterion(pred, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "id": "7SpqB1qQYnXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_epoch_ftt(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    writer: list,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "    dataset: str,\n",
        "    glob_silent: bool,\n",
        "    silent: bool\n",
        ") -> None:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "    \n",
        "    true_val = None\n",
        "    pred_val = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if dataset == 'train':\n",
        "            desc = 'loop over train batches'\n",
        "        else:\n",
        "            desc = 'loop over test batches'\n",
        "\n",
        "        if not glob_silent:\n",
        "            for i, (data, targets) in tqdm(\n",
        "                enumerate(dataloader),\n",
        "                total=len(dataloader),\n",
        "                desc=desc,\n",
        "            ):\n",
        "\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                data_num = data\n",
        "                data_cat = None\n",
        "                outputs = model(data_num, data_cat)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                epoch_loss.append(loss.item())\n",
        "\n",
        "                if true_val is None:\n",
        "                    true_val = targets\n",
        "                else:\n",
        "                    true_val = torch.cat((true_val, targets), 0)\n",
        "                \n",
        "                if pred_val is None:\n",
        "                    pred_val = outputs\n",
        "                else:\n",
        "                    pred_val = torch.cat((pred_val, outputs), 0)\n",
        "        else: \n",
        "            for i, (data, targets) in enumerate(dataloader):\n",
        "\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                data_num = data\n",
        "                data_cat = None\n",
        "                outputs = model(data_num, data_cat)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                epoch_loss.append(loss.item())\n",
        "\n",
        "                if true_val is None:\n",
        "                    true_val = targets\n",
        "                else:\n",
        "                    true_val = torch.cat((true_val, targets), 0)\n",
        "                \n",
        "                if pred_val is None:\n",
        "                    pred_val = outputs\n",
        "                else:\n",
        "                    pred_val = torch.cat((pred_val, outputs), 0)\n",
        "                    \n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs = pred_val,\n",
        "            targets = true_val\n",
        "        )\n",
        "        \n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "\n",
        "\n",
        "        loss = criterion(pred_val, true_val)\n",
        "        if dataset == 'test' and scheduler is not None:\n",
        "            scheduler.step(loss)\n",
        "        val_loss = loss.item()\n",
        "        \n",
        "        if not silent:\n",
        "            if dataset == 'train':\n",
        "                print(f'Train loss: {val_loss}\\n')\n",
        "            else:\n",
        "                print(f'Test loss: {val_loss}\\n')\n",
        "\n",
        "        writer.append((batch_metrics_list, val_loss))\n",
        "        \n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = metric_value_list[0]\n",
        "            if not silent:\n",
        "                if dataset == 'train':\n",
        "                    print(f'Train {metric_name}: {metric_value}\\n')\n",
        "                else:\n",
        "                    print(f'Test {metric_name}: {metric_value}\\n')"
      ],
      "metadata": {
        "id": "-AwrTMAxYquW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ftt(\n",
        "    n_epochs: int,\n",
        "    model: torch.nn.Module,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    val_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer_train: list,\n",
        "    writer_val: list,\n",
        "    writer_test: list,\n",
        "    device: torch.device,\n",
        "    patience=10,\n",
        "    silent=False,\n",
        "    glob_silent=False\n",
        ") -> None:\n",
        "    best_epoch = -1\n",
        "    best_metric = np.NINF\n",
        "    \n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        if not silent:\n",
        "            print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "\n",
        "        train_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            silent=silent\n",
        "        )\n",
        "        evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_train,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            dataset='train',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "        )\n",
        "        evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=val_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_val,\n",
        "            device=device,\n",
        "            epoch=epoch,\n",
        "            dataset='test',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "        )\n",
        "        temp_metric = writer_val[-1][0]['auc'][0]\n",
        "        if temp_metric > best_metric:\n",
        "            best_epoch = epoch\n",
        "            best_metric = temp_metric\n",
        "        elif epoch - best_epoch > patience:\n",
        "            break\n",
        "    evaluate_epoch_ftt(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            criterion=criterion,\n",
        "            scheduler=scheduler,\n",
        "            writer=writer_test,\n",
        "            device=device,\n",
        "            epoch=None,\n",
        "            dataset='test',\n",
        "            glob_silent=glob_silent,\n",
        "            silent=silent\n",
        "    )\n",
        "    auc = writer_test[-1][0]['auc'][0]\n",
        "    return auc\n",
        "    "
      ],
      "metadata": {
        "id": "VQiKVdBzYtSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "NwkJDncVYwTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Give Me Some Credit"
      ],
      "metadata": {
        "id": "K0ghjAF-Yz0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gsc = pd.read_csv('./gsc-training.csv', )\n",
        "df_gsc = df_gsc.drop(['Unnamed: 0'], axis=1)\n",
        "df_gsc.fillna(df_gsc.median(), inplace=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "nf = ['RevolvingUtilizationOfUnsecuredLines', 'DebtRatio', 'MonthlyIncome', 'age']\n",
        "cf = ['NumberOfTime30-59DaysPastDueNotWorse', 'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents']\n",
        "df_gsc[nf] = scaler.fit_transform(df_gsc[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_gsc[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_gsc.index\n",
        "df_gsc = df_gsc.drop(cf, axis=1)\n",
        "df_gsc = pd.concat([df_gsc, tdf], axis=1)\n",
        "df_gsc_target_name = 'SeriousDlqin2yrs'\n",
        "df_gsc.shape"
      ],
      "metadata": {
        "id": "5XC3edHPYx7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Churn Modelling"
      ],
      "metadata": {
        "id": "hsqQzr_IY1qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.read_csv('./Churn_Modelling.csv')\n",
        "df_cm = df_cm.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
        "df_cm.fillna(df_cm.median(), inplace=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "nf = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']\n",
        "cf = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n",
        "df_cm[nf] = scaler.fit_transform(df_cm[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_cm[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_cm.index\n",
        "df_cm = df_cm.drop(cf, axis=1)\n",
        "df_cm = pd.concat([df_cm, tdf], axis=1)\n",
        "\n",
        "df_cm_target_name = 'Exited'\n",
        "\n",
        "df_cm.shape"
      ],
      "metadata": {
        "id": "Ba2K41NYY3Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vehicle Loan Default"
      ],
      "metadata": {
        "id": "CLEd7r7xY4x8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vld = pd.read_csv('./vehicle_loan_default_train.csv')\n",
        "df_vld = df_vld.drop(['UNIQUEID', 'EMPLOYEE_CODE_ID', 'MOBILENO_AVL_FLAG'], axis=1)\n",
        "df_vld.fillna(df_vld.median(), inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_vld['AVERAGE_ACCT_AGE'] = le.fit_transform(df_vld['AVERAGE_ACCT_AGE'])\n",
        "df_vld['CREDIT_HISTORY_LENGTH'] = le.fit_transform(df_vld['CREDIT_HISTORY_LENGTH'])\n",
        "\n",
        "\n",
        "df_vld['DATE_OF_BIRTH'] = pd.to_datetime(df_vld['DATE_OF_BIRTH'], format='%d-%m-%Y')\n",
        "df_vld['DATE_OF_BIRTH_d'] = df_vld['DATE_OF_BIRTH'].dt.day\n",
        "df_vld['DATE_OF_BIRTH_m'] = df_vld['DATE_OF_BIRTH'].dt.month\n",
        "df_vld['DATE_OF_BIRTH_y'] = df_vld['DATE_OF_BIRTH'].dt.year\n",
        "df_vld = df_vld.drop(['DATE_OF_BIRTH'], axis=1)\n",
        "df_vld['DISBURSAL_DATE'] = pd.to_datetime(df_vld['DISBURSAL_DATE'], format='%d-%m-%Y')\n",
        "df_vld['DISBURSAL_DATE_d'] = df_vld['DISBURSAL_DATE'].dt.day\n",
        "df_vld['DISBURSAL_DATE_m'] = df_vld['DISBURSAL_DATE'].dt.month\n",
        "df_vld['DISBURSAL_DATE_y'] = df_vld['DISBURSAL_DATE'].dt.year\n",
        "df_vld = df_vld.drop(['DISBURSAL_DATE'], axis=1)\n",
        "\n",
        "\n",
        "nf = ['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'BRANCH_ID', 'SUPPLIER_ID', 'MANUFACTURER_ID', 'CURRENT_PINCODE_ID', \n",
        "     'PERFORM_CNS_SCORE', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT', 'SEC_CURRENT_BALANCE',\n",
        "     'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NO_OF_INQUIRIES', \n",
        "      'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', \n",
        "      'SEC_OVERDUE_ACCTS', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', \n",
        "     'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'DATE_OF_BIRTH_d', 'DATE_OF_BIRTH_m', 'DATE_OF_BIRTH_y', \n",
        "     'DISBURSAL_DATE_d', 'DISBURSAL_DATE_m', 'DISBURSAL_DATE_y'] \n",
        "cf = ['EMPLOYMENT_TYPE', 'STATE_ID', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG',\n",
        "     'PASSPORT_FLAG', 'PERFORM_CNS_SCORE_DESCRIPTION']\n",
        "scaler = StandardScaler()\n",
        "df_vld[nf] = scaler.fit_transform(df_vld[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_vld[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_vld.index\n",
        "df_vld = df_vld.drop(cf, axis=1)\n",
        "df_vld = pd.concat([df_vld, tdf], axis=1)\n",
        "\n",
        "df_vld_target_name = 'LOAN_DEFAULT'\n",
        "\n",
        "df_vld.shape"
      ],
      "metadata": {
        "id": "Ty6Ay6kjY6OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adult Income Dataset"
      ],
      "metadata": {
        "id": "DIy0fIXdY7w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ai = pd.read_csv('./adult.csv')\n",
        "df_ai.replace('<=50K', 0, inplace=True)\n",
        "df_ai.replace('>50K', 1, inplace=True)\n",
        "df_ai.fillna(df_ai.median(), inplace=True)\n",
        "nf = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week'] \n",
        "cf = ['workclass', 'education', 'educational-num', 'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "     'native-country']\n",
        "scaler = StandardScaler()\n",
        "df_ai[nf] = scaler.fit_transform(df_ai[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_ai[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_ai.index\n",
        "df_ai = df_ai.drop(cf, axis=1)\n",
        "df_ai = pd.concat([df_ai, tdf], axis=1)\n",
        "\n",
        "df_ai_target_name = 'income'\n",
        "df_ai.shape"
      ],
      "metadata": {
        "id": "7BiwANVgY9Td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HELOC"
      ],
      "metadata": {
        "id": "X9CSq4hbY_lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_heloc = pd.read_csv('./RiskData.csv')\n",
        "df_heloc.replace('.', 0, inplace=True)\n",
        "df_heloc = df_heloc.drop(['Sampling_Weight'], axis=1)\n",
        "\n",
        "nf = ['Interest_Revenue', 'Application_Date', 'Age', 'Income', 'Debt_Ratio', 'Loan_Amt',\n",
        "     'Loan_Amt_Req', 'LTV', 'FICO_Score', 'Prior_Custom_Score', 'Current_Custom_Score', 'CB_Age_Oldest_TL',\n",
        "     'CB_Age_Newest_TL', 'CB_Avg_Mos_File', 'CB_Nb_Sat_TL', 'CB_Pct_Sat_TL', 'CB_Mos_Since_Dlq', 'CB_Max_Dlq_12_Mos',\n",
        "     'CB_Max_Dlq_Ever', 'CB_Nb_Total_TL', 'CB_Nb_TL_Open_12', 'CB_Pct_IL_TL', 'CB_Nb_Inq_6_Mos', \n",
        "     'CB_Nb_Inq_6_Mos_excl_7_Days', 'CB_Rev_Util', 'CB_IL_Util', 'CB_Nb_Rev_TL_w_Bal', 'CB_Nb_IL_TL_w_Bal', \n",
        "     'CB_Nb_Rev_Tl_75_Pct_Limit', 'CB_Pct_TL_w_Bal']\n",
        "cf = ['Nb_Borrowers', 'Region', 'Bank_Relationship', 'CB_Nb_60_Plus_TL', 'CB_Nb_90_Plus_TL']\n",
        "scaler = StandardScaler()\n",
        "df_heloc.fillna(df_heloc.median(), inplace=True)\n",
        "df_heloc[nf] = df_heloc[nf].astype(float)\n",
        "df_heloc[cf] = df_heloc[cf].astype(str)\n",
        "\n",
        "df_heloc[nf] = scaler.fit_transform(df_heloc[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_heloc[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_heloc.index\n",
        "df_heloc = df_heloc.drop(cf, axis=1)\n",
        "df_heloc = pd.concat([df_heloc, tdf], axis=1)\n",
        "\n",
        "df_heloc_target_name = 'Risk_Flag'\n",
        "\n",
        "df_heloc.shape"
      ],
      "metadata": {
        "id": "ZRULeGouZAHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fraud Ecomm"
      ],
      "metadata": {
        "id": "Rnz9q_BGZBzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "df_fe = pd.read_csv('./fraud_ecomm.csv')\n",
        "ip = pd.read_csv('./IpAddress_to_Country.csv')\n",
        "df_fe['signup_time'] = pd.to_datetime(df_fe['signup_time'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_fe['signup_time_y'] = df_fe['signup_time'].dt.year\n",
        "df_fe['signup_time_mon'] = df_fe['signup_time'].dt.month\n",
        "df_fe['signup_time_w'] = df_fe['signup_time'].dt.week\n",
        "df_fe['signup_time_d'] = df_fe['signup_time'].dt.day\n",
        "df_fe['signup_time_h'] = df_fe['signup_time'].dt.hour\n",
        "df_fe['signup_time_m'] = df_fe['signup_time'].dt.minute\n",
        "df_fe['signup_time_s'] = df_fe['signup_time'].dt.second\n",
        "df_fe['signup_time_wd'] = df_fe['signup_time'].dt.dayofweek\n",
        "df_fe = df_fe.drop(['signup_time'], axis=1)\n",
        "df_fe['purchase_time'] = pd.to_datetime(df_fe['purchase_time'], format='%Y-%m-%d %H:%M:%S')\n",
        "df_fe['purchase_time_y'] = df_fe['purchase_time'].dt.year\n",
        "df_fe['purchase_time_mon'] = df_fe['purchase_time'].dt.month\n",
        "df_fe['purchase_time_w'] = df_fe['purchase_time'].dt.week\n",
        "df_fe['purchase_time_d'] = df_fe['purchase_time'].dt.day\n",
        "df_fe['purchase_time_h'] = df_fe['purchase_time'].dt.hour\n",
        "df_fe['purchase_time_m'] = df_fe['purchase_time'].dt.minute\n",
        "df_fe['purchase_time_s'] = df_fe['purchase_time'].dt.second\n",
        "df_fe['purchase_time_wd'] = df_fe['purchase_time'].dt.dayofweek\n",
        "df_fe = df_fe.drop(['purchase_time'], axis=1)\n",
        "le = LabelEncoder()\n",
        "df_fe['device_id'] = le.fit_transform(df_fe['device_id'])\n",
        "df_fe['source'] = le.fit_transform(df_fe['source'])\n",
        "df_fe['browser'] = le.fit_transform(df_fe['browser'])\n",
        "df_fe['sex'] = le.fit_transform(df_fe['sex'])\n",
        "df_fe['age'] = le.fit_transform(df_fe['age'])\n",
        "ip['country'] = le.fit_transform(ip['country'])\n",
        "ip['lower_bound_ip_address'] = ip['lower_bound_ip_address'].astype('float')\n",
        "ip['upper_bound_ip_address'] = ip['upper_bound_ip_address'].astype('float')\n",
        "df_fe['ip_address'] = df_fe['ip_address'].astype('float')\n",
        "def ip_to_country(ip_val):\n",
        "    try :\n",
        "        return ip.country[(ip.lower_bound_ip_address < ip_val)                            \n",
        "                                & \n",
        "                                (ip.upper_bound_ip_address > ip_val)].iloc[0]\n",
        "    except IndexError :\n",
        "        return -1\n",
        "df_fe['ip_country'] = df_fe['ip_address'].apply(ip_to_country)\n",
        "\n",
        "device_duplicates = pd.DataFrame(df_fe.groupby(by = \"device_id\").device_id.count())\n",
        "device_duplicates.rename(columns={\"device_id\": \"freq_device\"}, inplace=True)           \n",
        "device_duplicates.reset_index(level=0, inplace= True)\n",
        "df_fe = df_fe.merge(device_duplicates, on= \"device_id\")\n",
        "\n",
        "df_fe = df_fe.drop(['user_id'], axis=1)\n",
        "nf = ['purchase_value', 'ip_address', 'device_id', 'signup_time_y', 'signup_time_mon',\n",
        "       'signup_time_w', 'signup_time_d', 'signup_time_h', 'signup_time_m',\n",
        "       'signup_time_s', 'signup_time_wd', 'purchase_time_y',\n",
        "       'purchase_time_mon', 'purchase_time_w', 'purchase_time_d',\n",
        "       'purchase_time_h', 'purchase_time_m', 'purchase_time_s',\n",
        "       'purchase_time_wd', 'age', 'ip_country']\n",
        "cf = ['source', 'browser', 'sex']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_fe[nf] = scaler.fit_transform(df_fe[nf])\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "transf = ohe.fit_transform(df_fe[cf])\n",
        "tdf = pd.DataFrame(transf, columns=ohe.get_feature_names())\n",
        "tdf.index = df_fe.index\n",
        "df_fe = df_fe.drop(cf, axis=1)\n",
        "df_fe = pd.concat([df_fe, tdf], axis=1)\n",
        "\n",
        "df_fe_target_name = 'class'\n",
        "\n",
        "df_fe.shape"
      ],
      "metadata": {
        "id": "aVPHE2jhZDY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runs"
      ],
      "metadata": {
        "id": "mGbsmNecZE4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = {'gsc': df_gsc, 'cm': df_cm, 'heloc': df_heloc, 'vld': df_vld, 'ai': df_ai, 'fe': df_fe}\n",
        "dfs_shapes = {'gsc': df_gsc.shape[1]-1, 'cm': df_cm.shape[1]-1, \n",
        "              'heloc': df_heloc.shape[1]-1, 'vld': df_vld.shape[1]-1, 'ai': df_ai.shape[1]-1, 'fe': df_fe.shape[1]-1}\n",
        "dfs_targets = {'gsc': df_gsc_target_name, 'cm': df_cm_target_name, \n",
        "               'heloc': df_heloc_target_name, 'vld': df_vld_target_name,\n",
        "              'ai': df_ai_target_name, 'fe': df_fe_target_name}\n",
        "dfs_names = ['gsc', 'cm', 'vld', 'ai', 'heloc', 'fe']\n",
        "preproc_types = ['Periodic', 'Fourier', 'Linear', 'AutoDis', 'Tokens', 'SoftEmbedding']"
      ],
      "metadata": {
        "id": "nxBsZn68ZGS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_gsc.shape)\n",
        "print(df_cm.shape)\n",
        "print(df_vld.shape)\n",
        "print(df_ai.shape)\n",
        "print(df_heloc.shape)\n",
        "print(df_fe.shape)"
      ],
      "metadata": {
        "id": "v6u5yGSZZHxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preproc_params(dataset, preproc_type, tf, d_embedding):\n",
        "    preproc_params = {}\n",
        "    if preproc_type == 'Periodic':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'n': d_embedding, 'sigma': 0.1, 'trainable': True,\n",
        "                         'initialization': 'normal', 'tf': tf}\n",
        "        preproc_name = 'Periodic'\n",
        "    elif preproc_type == 'Fourier':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'n': d_embedding, 'sigma': 0.1, 'trainable': False,\n",
        "                         'initialization': 'normal', 'tf': tf}\n",
        "        preproc_name = 'Periodic'\n",
        "    elif preproc_type == 'Linear':\n",
        "        preproc_params = {'n_layers': 2, 'n_features': dfs_shapes[dataset], \n",
        "                          'd_embeddings': [d_embedding, d_embedding], 'tf': tf}\n",
        "        preproc_name = 'Linear'\n",
        "    elif preproc_type == 'AutoDis':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'd_embedding': d_embedding, 'n_meta_embeddings': 20,\n",
        "                         'temperature': 0.5, 'tf': tf}\n",
        "        preproc_name = 'AutoDis'\n",
        "    elif preproc_type == 'Tokens' or preproc_type == 'ARM':\n",
        "        preproc_params = {'n_features': dfs_shapes[dataset], 'd_embedding': d_embedding, 'tf': tf}\n",
        "        preproc_name = preproc_type\n",
        "    elif preproc_type == 'SoftEmbedding':\n",
        "        preproc_params = {'num_embeddings': dfs_shapes[dataset], 'embeddings_dim': d_embedding, \n",
        "                          'emb_initializer': None, 'tf': tf}\n",
        "        preproc_name = 'SoftEmbedding'\n",
        "    elif preproc_type == 'BinEncoding' or preproc_type == 'ARM_Bin':\n",
        "        preproc_params = {'bin_edges': bin_edges, 'bins': bins, 'bin_values': bin_values, 'nbins': nbins,\n",
        "                         'tf': tf, 'd_token': d_embedding, 'bias': True, 'initialization': 'normal', \n",
        "                          'device': device}\n",
        "        preproc_name = 'BinEncoding'\n",
        "    return preproc_params, preproc_name"
      ],
      "metadata": {
        "id": "K8XMNPDzZJE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './output/bc/mlp/'\n",
        "positional = False\n",
        "tf = False\n",
        "d_embedding = 10"
      ],
      "metadata": {
        "id": "9AF9_aV9ZMGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "3vlSY1CXZOhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for df_name in tqdm(['gsc']):\n",
        "    for preproc_type in tqdm(preproc_types):\n",
        "        temp_path = df_name + '/' + preproc_type + '/'\n",
        "        X = dfs[df_name].drop([dfs_targets[df_name]], axis=1)\n",
        "        y = dfs[df_name][dfs_targets[df_name]]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "        X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
        "        temp = []\n",
        "        final_res = defaultdict(list)\n",
        "        n_runs=15\n",
        "        silent=True,\n",
        "        glob_silent=True\n",
        "        best_metric = np.NINF\n",
        "        cat_len = 0\n",
        "        seeds = np.random.randint(1, 100500, 15)\n",
        "        for i in range(n_runs):\n",
        "            if not silent:\n",
        "                print(f'Run {i+1}:')\n",
        "                print()\n",
        "            set_global_seed(seeds[i])\n",
        "            res_metrics = defaultdict(list)   \n",
        "            X_train_t = torch.from_numpy(X_train_t.values).float()\n",
        "            y_train_t = torch.from_numpy(y_train_t.values.ravel()).float().unsqueeze(1)\n",
        "            X_test_t = torch.from_numpy(X_test_t.values).float()\n",
        "            y_test_t = torch.from_numpy(y_test_t.values.ravel()).float().unsqueeze(1)\n",
        "            X_test = torch.from_numpy(X_test.values).float()\n",
        "            y_test = torch.from_numpy(y_test.values.ravel()).float().unsqueeze(1)\n",
        "        \n",
        "            train_ds = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
        "            val_ds = torch.utils.data.TensorDataset(X_test_t, y_test_t)\n",
        "            test_ds = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "            train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=256, shuffle=True)\n",
        "            val_dataloader = torch.utils.data.DataLoader(val_ds, batch_size=256, shuffle=True)\n",
        "            test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=256, shuffle=True)\n",
        "    \n",
        "            writer_train = []\n",
        "            writer_val = []\n",
        "            writer_test = []\n",
        "      \n",
        "        \n",
        "            metric_factory = MetricFactory()\n",
        "            preproc_params, preproc_name = get_preproc_params(df_name, preproc_type, tf, d_embedding)\n",
        "                \n",
        "                \n",
        "            preproc_params['tf'] = True\n",
        "            feature_tokenizer = FeatureTokenizer(X.shape[1], None, d_embedding, \n",
        "                                                     preproc_name, 'TT', preproc_params, positional, tf)\n",
        "            model = MLP(n_in=dfs_shapes[df_name], hidden_size=[512, 256, 128], drop_rate=[0.1, 0.1, 0.1], \n",
        "                device=device, use_bn=True, use_noise=True, use_dropout=True,\n",
        "                preproc=False, preproc_type=preproc_name, preproc_args=preproc_params,\n",
        "                        positional=positional, tokenizer=feature_tokenizer).to(device)\n",
        "\n",
        "                \n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=0)\n",
        "            \n",
        "            criterion = torch.nn.BCEWithLogitsLoss()\n",
        "                \n",
        "            metric = train_ftt(100, model, train_dataloader, val_dataloader, test_dataloader, optimizer, None, criterion, writer_train,\n",
        "                           writer_val, writer_test, device, 20, silent, glob_silent)\n",
        "            test_metrics = writer_test[-1][0]\n",
        "            test_loss = writer_test[-1][1]\n",
        "            for key, value in test_metrics.items():\n",
        "                res_metrics[key].append(value[0])\n",
        "            res_metrics['loss'].append(test_loss)\n",
        "            for key, value in res_metrics.items():\n",
        "                final_res[key].append(np.mean(value))\n",
        "            s = 'run' + str(i + 1) + '.pickle'\n",
        "            with open(path + temp_path + s, 'wb') as f:\n",
        "                pickle.dump(res_metrics, f)\n",
        "    \n",
        "        ans = {}\n",
        "        for key, value in final_res.items():\n",
        "            ans[key] = float(np.mean(value))\n",
        "\n",
        "        with open(path + temp_path + 'metrics.yml', 'w') as f:\n",
        "            ruamel.yaml.round_trip_dump(ans, f)"
      ],
      "metadata": {
        "id": "Fx1JoaDsZPfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}